{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narwhals Machine Learning Patterns\n",
    "\n",
    "Narwhals provides a unified interface for DataFrame operations across Pandas, Polars, and Dask. This tutorial outlines patterns for building backend-agnostic functions tailored to typical machine learning workflows, including data validation, feature engineering, and time series processing. These patterns help create scalable and maintainable pipelines suitable for both prototyping and production, benefiting both OSS and enterprise-grade developers.\n",
    "\n",
    "## Overview of Patterns\n",
    "\n",
    "1. **Backward Compatability Policy**:  \n",
    "   Narwhals ensures stability for library maintainers with strict backward compatibility. Code written with `import narwhals.stable.v1 as nw` will remain functional indefinitely, even if breaking changes occur. Breaking changes are isolated in `narwhals.stable.v2` or higher, while `narwhals.stable.v1` remains unaffected, enabling safe coexistence of multiple versions.\n",
    "\n",
    "2. **Data Validation**:  \n",
    "   Use `@nw.narwhalify` to decorate functions for consistent backend-agnostic validation. Leverage `nw.col(...)` for type casting, null handling, and basic statistics across backends.\n",
    "3. **Feature Engineering**:  \n",
    "   Chain transformations like `.cast(nw.Float64())` or `.fill_null(...)` to preprocess numeric and categorical data. Defer materialization until necessary to optimize memory usage.\n",
    "4. **Time Series Handling**:  \n",
    "   Validate temporal data by grouping with `.group_by([id_col, time_col])`, checking for uniqueness or applying rolling windows. Maintain backend independence without additional logic.\n",
    "5. **Workflow Optimization**:  \n",
    "   Begin with lazy mode (e.g., `nw.from_native(dask_df)`) for scalability and switch to eager mode using `.collect()` or `.to_native()` for tasks needing immediate results.\n",
    "6. **OSS and Enterprise Notes**:  \n",
    "   Use tools like Hatch to manage lean environments for production and comprehensive setups for testing.  \n",
    "   - Define lean setups in `[tool.hatch.envs.default]` for minimal dependencies (e.g., Narwhals and Pandas).  \n",
    "   - Use `[tool.hatch.envs.test]` for broader testing across multiple backends (e.g., Polars and Dask).  \n",
    "   Handle unsupported objects gracefully by setting `pass_through=True` or `strict=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import narwhals as nw\n",
    "from narwhals.typing import FrameT\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Union, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 1: Backward Compatibility Policy\n",
    "\n",
    "This pattern demonstrates how Narwhals guarantees **backward compatibility**, ensuring stability for production-grade workflows and eliminating breaking changes across updates.\n",
    "\n",
    "1. **Stable Namespace**: Code written with `narwhals.stable.v1` remains functional indefinitely, even if breaking changes occur in Narwhals or its backends.\n",
    "2. **Version Migration**: Developers can adopt new features by explicitly switching to updated namespaces, such as `narwhals.stable.v2`.\n",
    "3. **Integration**: Multiple Narwhals versions can coexist in a single project, ensuring smooth collaboration without dependency conflicts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result using Narwhals stable API v1:\n",
      "   mean_feature1  sum_feature2\n",
      "0            2.0            15\n"
     ]
    }
   ],
   "source": [
    "# Import the stable API for guaranteed compatibility\n",
    "import narwhals.stable.v1 as nw\n",
    "from narwhals.typing import IntoFrameT\n",
    "\n",
    "# Example dataset\n",
    "data = {\"feature1\": [1, 2, 3], \"feature2\": [4, 5, 6]}\n",
    "\n",
    "# Workflow using Narwhals stable.v1\n",
    "def backward_compatible_workflow(df: IntoFrameT) -> IntoFrameT:\n",
    "    \"\"\"Use Narwhals stable.v1 API to process data.\"\"\"\n",
    "    # Convert to Narwhals lazy frame\n",
    "    df_nw = nw.from_native(df)\n",
    "    \n",
    "    # Perform transformations\n",
    "    df_transformed = df_nw.select([\n",
    "        nw.col(\"feature1\").mean().alias(\"mean_feature1\"),\n",
    "        nw.col(\"feature2\").sum().alias(\"sum_feature2\"),\n",
    "    ])\n",
    "    \n",
    "    # Convert back to native format (e.g., Pandas)\n",
    "    return df_transformed.to_native()\n",
    "\n",
    "# Testing the backward-compatible workflow\n",
    "import pandas as pd\n",
    "df_pd = pd.DataFrame(data)\n",
    "result = backward_compatible_workflow(df_pd)\n",
    "\n",
    "print(\"Result using Narwhals stable API v1:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 1: Lazy-to-Eager Frame Transitions\n",
    "\n",
    "This pattern demonstrates how Narwhals provides a unified interface for transitioning between lazy and eager evaluation, regardless of the backend. There's no need to distinguish between different materialization methods - Narwhals handles this automatically. The example shows:\n",
    "\n",
    "- **Unified Collection**: Using `collect()` provides a consistent way to materialize results, automatically handling the transition from any lazy backend (like Dask) to an eager Pandas DataFrame\n",
    "- **Lazy Operations**: Start with any lazy backend for memory-efficient processing of large datasets, with operations optimized and deferred until needed\n",
    "- **Backend Transitions**: Narwhals automatically manages the transition from lazy (e.g., Dask) to eager (Pandas) evaluation, simplifying ML workflows\n",
    "- **ML Integration**: Final `to_native()` call provides the DataFrame in the format needed by ML libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dask DataFrame (lazy):\n",
      "Dask DataFrame Structure:\n",
      "              numeric_feature categorical_feature\n",
      "npartitions=2                                    \n",
      "0                     float64              string\n",
      "3                         ...                 ...\n",
      "4                         ...                 ...\n",
      "Dask Name: frompandas, 1 expression\n",
      "Expr=df\n",
      "\n",
      "Processed Pandas DataFrame (eager):\n",
      "   numeric_feature categorical_feature\n",
      "0              1.5                   A\n",
      "1              2.0                   B\n",
      "2              0.0                   A\n",
      "3              4.0                   C\n",
      "4              5.5                   B\n"
     ]
    }
   ],
   "source": [
    "# Create sample ML dataset\n",
    "data = {\n",
    "    \"numeric_feature\": [1.5, 2.0, None, 4.0, 5.5],    # Has missing value\n",
    "    \"categorical_feature\": [\"A\", \"B\", \"A\", \"C\", \"B\"]  # Needs encoding\n",
    "}\n",
    "df_pd = pd.DataFrame(data)\n",
    "\n",
    "# Start with any lazy backend (e.g., Dask)\n",
    "df_dask = dd.from_pandas(df_pd, npartitions=2)\n",
    "\n",
    "# Unified lazy-to-eager pattern\n",
    "df_nw = nw.from_native(df_dask)           # Works with any lazy frame\n",
    "df_processed = df_nw.select([             # Lazy operations\n",
    "    nw.col(\"numeric_feature\")\n",
    "       .fill_null(0)\n",
    "       .cast(nw.Float64())\n",
    "       .alias(\"numeric_feature\"),\n",
    "    nw.col(\"categorical_feature\")\n",
    "       .cast(nw.String())\n",
    "       .alias(\"categorical_feature\")\n",
    "])\n",
    "df_collected = df_processed.collect()      # Unified collection - no compute() needed\n",
    "df_pandas = df_collected.to_native()       # Ready for ML libraries\n",
    "\n",
    "print(\"Original Dask DataFrame (lazy):\")\n",
    "print(df_dask)\n",
    "print(\"\\nProcessed Pandas DataFrame (eager):\")\n",
    "print(df_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 2: Data Validation - Handling Unsupported Types\n",
    "\n",
    "This pattern demonstrates how to build robust data validation pipelines that can handle unexpected or unsupported data types. In data engineering workflows, you often need to validate and process data from various sources that may contain custom objects, mixed types, or other non-standard formats. The pass_through parameter provides explicit control over validation behavior:\n",
    "\n",
    "- **Development Mode**: Using `pass_through=True` enables initial data exploration and debugging by allowing inspection of problematic data types. This is crucial when investigating data quality issues or understanding new data sources.\n",
    "- **Production Mode**: Using `pass_through=False` (default) enforces strict validation rules in production pipelines, preventing unexpected data types from silently propagating through the system and causing downstream issues.\n",
    "- **Error Recovery**: Both modes provide clear error messages that help identify and handle data quality issues at the appropriate stage of your pipeline.\n",
    "- **Pipeline Integration**: Choose development mode during data exploration and pipeline development, then switch to production mode for robust, production-grade data validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development Mode (pass_through=True):\n",
      "Expected: Can load and view all data\n",
      "   feature1 unsupported\n",
      "0         1   Custom(1)\n",
      "1         2   Custom(2)\n",
      "2         3   Custom(3)\n",
      "\n",
      "Trying operations on normal column:\n",
      "Expected: Should work normally\n",
      "   feature1\n",
      "0       1.0\n",
      "1       2.0\n",
      "2       3.0\n",
      "\n",
      "Trying operations on unsupported column:\n",
      "Expected: Should fail gracefully\n",
      "Error: Cannot cast custom objects to Float64\n",
      "\n",
      "Production Mode (pass_through=False):\n",
      "Expected: Should fail on unsupported types\n",
      "Error: Unsupported types not allowed in strict mode\n"
     ]
    }
   ],
   "source": [
    "class CustomObject:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def __str__(self):\n",
    "        return f\"Custom({self.value})\"\n",
    "\n",
    "# Create DataFrame with unsupported objects\n",
    "data = {\n",
    "    \"feature1\": [1, 2, 3],\n",
    "    \"unsupported\": [CustomObject(1), CustomObject(2), CustomObject(3)]\n",
    "}\n",
    "df_pd = pd.DataFrame(data)\n",
    "\n",
    "# Development mode - allows inspection\n",
    "print(\"Development Mode (pass_through=True):\")\n",
    "print(\"Expected: Can load and view all data\")\n",
    "df_nw1 = nw.from_native(df_pd, pass_through=True)\n",
    "print(df_nw1.to_native())\n",
    "\n",
    "print(\"\\nTrying operations on normal column:\")\n",
    "print(\"Expected: Should work normally\")\n",
    "result = df_nw1.select([\n",
    "    nw.col(\"feature1\").cast(nw.Float64())\n",
    "])\n",
    "print(result.to_native())\n",
    "\n",
    "print(\"\\nTrying operations on unsupported column:\")\n",
    "print(\"Expected: Should fail gracefully\")\n",
    "try:\n",
    "    result = df_nw1.select([\n",
    "        nw.col(\"unsupported\").cast(nw.Float64())\n",
    "    ])\n",
    "except Exception as e:\n",
    "    print(f\"Error: Cannot cast custom objects to Float64\")\n",
    "\n",
    "# Production mode - strict type checking\n",
    "print(\"\\nProduction Mode (pass_through=False):\")\n",
    "print(\"Expected: Should fail on unsupported types\")\n",
    "try:\n",
    "    df_nw2 = nw.from_native(df_pd, pass_through=False)\n",
    "    result = df_nw2.select([\n",
    "        nw.col(\"unsupported\").cast(nw.Float64())\n",
    "    ])\n",
    "except Exception as e:\n",
    "    print(\"Error: Unsupported types not allowed in strict mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 3: Data Validation\n",
    "\n",
    "This pattern shows how to validate data types and quality across DataFrame operations in a backend-agnostic way. By using Narwhals native types and operations, you can ensure consistent validation behavior regardless of the underlying DataFrame implementation. The example shows:\n",
    "\n",
    "- **Type Safety**: Using Narwhals native types (Float64, String) ensures consistent type handling across backends, preventing type-related errors in ML pipelines\n",
    "- **Validation Workflow**: Backend-agnostic operations for checking nulls, type compatibility, and data quality enable robust validation pipelines\n",
    "- **Error Handling**: Graceful error recovery and clear error messages help identify data quality issues early in the pipeline\n",
    "- **ML Integration**: Consistent validation behavior across training and inference ensures reliable model deployment\n",
    "\n",
    "Note: Dask backend requires different handling for lazy evaluation. See Pattern 1 for lazy-to-eager transition patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Pandas backend:\n",
      "==================================================\n",
      "\n",
      "Validating numeric column with nulls:\n",
      "Expected: Should compute mean and null count\n",
      "Success - Mean: 2.3333333333333335\n",
      "Success - Null count: 1\n",
      "\n",
      "Trying to convert invalid strings to float:\n",
      "Expected: Should fail with type conversion error\n",
      "Failed as expected: could not convert string to float: 'bad'\n",
      "\n",
      "Testing Polars backend:\n",
      "==================================================\n",
      "\n",
      "Validating numeric column with nulls:\n",
      "Expected: Should compute mean and null count\n",
      "Success - Mean: 2.3333333333333335\n",
      "Success - Null count: 1\n",
      "\n",
      "Trying to convert invalid strings to float:\n",
      "Expected: Should fail with type conversion error\n",
      "Failed as expected: conversion from `str` to `f64` failed in column 'mixed' for 1 out of 4 values: [\"bad\"]\n"
     ]
    }
   ],
   "source": [
    "# Create sample data with validation issues\n",
    "data = {\n",
    "    'numeric': [1, 2, None, 4],           # Has null\n",
    "    'mixed': ['1', '2', 'bad', '4'],      # Has invalid value\n",
    "}\n",
    "\n",
    "# Test across backends\n",
    "backends = {\n",
    "    'Pandas': pd.DataFrame(data),\n",
    "    'Polars': pl.DataFrame(data)\n",
    "}\n",
    "\n",
    "for name, df in backends.items():\n",
    "    print(f\"\\nTesting {name} backend:\")\n",
    "    print(\"=\" * 50)\n",
    "    df_nw = nw.from_native(df)\n",
    "    \n",
    "    # Validate numeric column (should succeed with nulls)\n",
    "    print(\"\\nValidating numeric column with nulls:\")\n",
    "    print(\"Expected: Should compute mean and null count\")\n",
    "    try:\n",
    "        result = df_nw.select([\n",
    "            nw.col(\"numeric\")\n",
    "               .cast(nw.Float64())\n",
    "               .mean()\n",
    "               .alias(\"mean\"),\n",
    "            nw.col(\"numeric\")\n",
    "               .is_null()\n",
    "               .sum()\n",
    "               .alias(\"nulls\")\n",
    "        ])\n",
    "        print(f\"Success - Mean: {result['mean'].item()}\")\n",
    "        print(f\"Success - Null count: {result['nulls'].item()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed as expected: {str(e)}\")\n",
    "    \n",
    "    # Try invalid column (should fail with type error)\n",
    "    print(\"\\nTrying to convert invalid strings to float:\")\n",
    "    print(\"Expected: Should fail with type conversion error\")\n",
    "    try:\n",
    "        result = df_nw.select([\n",
    "            nw.col(\"mixed\")\n",
    "               .cast(nw.Float64())\n",
    "               .mean()\n",
    "        ])\n",
    "        print(\"Unexpected success!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed as expected: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 4: Feature Engineering & Collect-Then-Item Pattern\n",
    "\n",
    "This pattern demonstrates how to build efficient feature engineering pipelines for ML workflows. A critical challenge in ML pipelines is handling both lazy backends (like Dask for large datasets) and eager backends (like Pandas for interactive development). The key is following a consistent materialization pattern when you need concrete values (like computing means for imputation).\n",
    "\n",
    "Key principles for consistent feature engineering:\n",
    "- **Consistent Materialization**: Always check if collect() is needed before item() - use `result.item()` for eager frames and `result.collect().item()` for lazy frames. This pattern ensures your code works correctly whether using Dask for large-scale processing or Pandas for development.\n",
    "- **Evaluation Strategy**: Use `eager_only=True` for functions that compute statistics (means, medians, etc.) since these require materialized values. Keep other transformations (type casting, null filling) lazy to let Narwhals optimize them.\n",
    "- **Backend Independence**: By following the collect-then-item pattern, your functions work automatically with any backend - they'll use collect() when needed (Dask) and skip it when unnecessary (Pandas).\n",
    "- **Memory Efficiency**: Only use the materialization pattern when you absolutely need concrete values (like computing statistics). Let other operations stay lazy so Narwhals can optimize them together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pandas backend:\n",
      "==================================================\n",
      "\n",
      "Processing numeric feature:\n",
      "Expected: Nulls filled with mean value, cast to Float64\n",
      "   integer_feature\n",
      "0              1.0\n",
      "1              2.0\n",
      "2              3.0\n",
      "3              4.0\n",
      "4              5.0\n",
      "\n",
      "Processing categorical feature:\n",
      "Expected: Uppercase strings, nulls filled with UNKNOWN\n",
      "  category_messy\n",
      "0              A\n",
      "1              B\n",
      "2        UNKNOWN\n",
      "3              C\n",
      "4              B\n",
      "\n",
      "Polars backend:\n",
      "==================================================\n",
      "\n",
      "Processing numeric feature:\n",
      "Expected: Nulls filled with mean value, cast to Float64\n",
      "shape: (5, 1)\n",
      "┌─────────────────┐\n",
      "│ integer_feature │\n",
      "│ ---             │\n",
      "│ f64             │\n",
      "╞═════════════════╡\n",
      "│ 1.0             │\n",
      "│ 2.0             │\n",
      "│ 3.0             │\n",
      "│ 4.0             │\n",
      "│ 5.0             │\n",
      "└─────────────────┘\n",
      "\n",
      "Processing categorical feature:\n",
      "Expected: Uppercase strings, nulls filled with UNKNOWN\n",
      "shape: (5, 1)\n",
      "┌────────────────┐\n",
      "│ category_messy │\n",
      "│ ---            │\n",
      "│ str            │\n",
      "╞════════════════╡\n",
      "│ A              │\n",
      "│ B              │\n",
      "│ UNKNOWN        │\n",
      "│ C              │\n",
      "│ B              │\n",
      "└────────────────┘\n",
      "\n",
      "Dask backend:\n",
      "==================================================\n",
      "\n",
      "Processing numeric feature:\n",
      "Expected: Nulls filled with mean value, cast to Float64\n",
      "Dask DataFrame Structure:\n",
      "              integer_feature\n",
      "npartitions=2                \n",
      "0                     float64\n",
      "3                         ...\n",
      "4                         ...\n",
      "Dask Name: getitem, 9 expressions\n",
      "Expr=Assign(frame=df)[['integer_feature']]\n",
      "\n",
      "Processing categorical feature:\n",
      "Expected: Uppercase strings, nulls filled with UNKNOWN\n",
      "Dask DataFrame Structure:\n",
      "              category_messy\n",
      "npartitions=2               \n",
      "0                     string\n",
      "3                        ...\n",
      "4                        ...\n",
      "Dask Name: getitem, 11 expressions\n",
      "Expr=Assign(frame=df)[['category_messy']]\n"
     ]
    }
   ],
   "source": [
    "# Create sample data with preprocessing needs\n",
    "data = {\n",
    "    'integer_feature': [1, 2, None, 4, 5],           # Needs mean imputation\n",
    "    'category_messy': ['a', 'B', None, 'c', 'b']     # Needs standardization\n",
    "}\n",
    "\n",
    "# Test across backends\n",
    "backends = {\n",
    "    'Pandas': pd.DataFrame(data),\n",
    "    'Polars': pl.DataFrame(data),\n",
    "    'Dask': dd.from_pandas(pd.DataFrame(data), npartitions=2)  # Lazy Dask frame\n",
    "}\n",
    "\n",
    "@nw.narwhalify(eager_only=True)  # Decorator handles materialization consistently\n",
    "def process_numeric_feature(df: FrameT, column: str) -> FrameT:\n",
    "    \"\"\"Process a numeric feature for ML.\n",
    "    \n",
    "    Common ML transformations:\n",
    "    - Convert to float\n",
    "    - Fill nulls with mean\n",
    "    - Standardize format\n",
    "    \"\"\"\n",
    "    # Get mean first - handle both lazy and eager cases\n",
    "    result = df.select([\n",
    "        nw.col(column)\n",
    "           .cast(nw.Float64())\n",
    "           .mean()\n",
    "    ])\n",
    "    mean_val = result.item() if not hasattr(result, 'collect') else result.collect().item()\n",
    "    \n",
    "    # Then use it for filling nulls\n",
    "    return df.select([\n",
    "        nw.col(column)\n",
    "           .cast(nw.Float64())\n",
    "           .fill_null(mean_val)\n",
    "           .alias(column)\n",
    "    ])\n",
    "\n",
    "@nw.narwhalify  # Default lazy evaluation for chainable operations\n",
    "def process_categorical_feature(df: FrameT, column: str) -> FrameT:\n",
    "    \"\"\"Process a categorical feature for ML.\n",
    "    \n",
    "    Common ML transformations:\n",
    "    - Handle nulls first\n",
    "    - Standardize case\n",
    "    - Ensure string format\n",
    "    \"\"\"\n",
    "    return df.select([\n",
    "        nw.col(column)\n",
    "           .fill_null(\"UNKNOWN\")     # Handle nulls before casting\n",
    "           .cast(nw.String())\n",
    "           .str.to_uppercase()\n",
    "           .alias(column)\n",
    "    ])\n",
    "\n",
    "# Test features across backends\n",
    "for name, df in backends.items():\n",
    "    print(f\"\\n{name} backend:\")\n",
    "    print(\"=\" * 50)\n",
    "    df_nw = nw.from_native(df)  # Works with any backend (lazy or eager)\n",
    "    \n",
    "    print(\"\\nProcessing numeric feature:\")\n",
    "    print(\"Expected: Nulls filled with mean value, cast to Float64\")\n",
    "    numeric_result = process_numeric_feature(df_nw, \"integer_feature\")\n",
    "    print(numeric_result)\n",
    "    \n",
    "    print(\"\\nProcessing categorical feature:\")\n",
    "    print(\"Expected: Uppercase strings, nulls filled with UNKNOWN\")\n",
    "    categorical_result = process_categorical_feature(df_nw, \"category_messy\")\n",
    "    print(categorical_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 5: Time Series Validation & Materialization\n",
    "\n",
    "This pattern demonstrates how to validate temporal data quality in ML/DS workflows. Time series data processing often requires complex transformations (interpolation, resampling, windowing) that need to work efficiently across different scales - from interactive analysis in Pandas to large-scale processing in Dask. By handling lazy and eager evaluation consistently, you can write temporal validation code once and use it across your entire ML pipeline.\n",
    "\n",
    "Key principles for time series validation:\n",
    "- **Consistent Materialization**: Follow the collect-then-item pattern when materializing results, ensuring consistent behavior across Pandas (eager) and Dask (lazy) backends\n",
    "- **Entity Grouping**: Handle multiple time series efficiently by grouping operations, letting Narwhals optimize the execution plan\n",
    "- **Data Quality**: Detect and report duplicate timestamps that could skew temporal analysis, using lazy evaluation for memory efficiency\n",
    "- **Backend Independence**: Write validation functions that work identically whether processing historical data with Dask or real-time data with Pandas\n",
    "\n",
    "The output shows the same validation working seamlessly across backends (Pandas, Polars, Dask), with each backend displaying results in its native format while maintaining consistent behavior. This enables developers to focus on temporal logic rather than backend-specific implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Uniqueness Validation Results\n",
      "==================================================\n",
      "\n",
      "Pandas Backend Results:\n",
      "------------------------------\n",
      "   id  timestamp  count\n",
      "0   1 2023-01-01      2\n",
      "\n",
      "Polars Backend Results:\n",
      "------------------------------\n",
      "shape: (1, 3)\n",
      "┌─────┬─────────────────────┬───────┐\n",
      "│ id  ┆ timestamp           ┆ count │\n",
      "│ --- ┆ ---                 ┆ ---   │\n",
      "│ i64 ┆ datetime[μs]        ┆ u32   │\n",
      "╞═════╪═════════════════════╪═══════╡\n",
      "│ 1   ┆ 2023-01-01 00:00:00 ┆ 2     │\n",
      "└─────┴─────────────────────┴───────┘\n",
      "\n",
      "Dask Backend Results:\n",
      "------------------------------\n",
      "Dask DataFrame Structure:\n",
      "                  id       timestamp  count\n",
      "npartitions=1                              \n",
      "               int64  datetime64[ns]  int64\n",
      "                 ...             ...    ...\n",
      "Dask Name: loc, 10 expressions\n",
      "Expr=Loc(frame=ResetIndex(frame=ColumnsSetter(frame=(GroupbyAggregation(frame=df, arg=defaultdict(<class 'list'>, {'timestamp': ['count']}), observed=True, dropna=False))[[('timestamp', 'count')]], columns=('count',))), iindexer=RenameSeries(frame=RenameSeries(frame=(ResetIndex(frame=ColumnsSetter(frame=(GroupbyAggregation(frame=df, arg=defaultdict(<class 'list'>, {'timestamp': ['count']}), observed=True, dropna=False))[[('timestamp', 'count')]], columns=('count',))))['count'] > 1, index='count'), index='count'))\n"
     ]
    }
   ],
   "source": [
    "# Generate hourly timestamps with duplicates\n",
    "base_timestamps = pd.date_range(\n",
    "    start=\"2023-01-01\",\n",
    "    periods=3,\n",
    "    freq=\"h\"\n",
    ")\n",
    "timestamps = [\n",
    "    base_timestamps[0],  # First timestamp\n",
    "    base_timestamps[0],  # Duplicate for id=1\n",
    "    base_timestamps[1],\n",
    "    base_timestamps[0],  # Duplicate for id=2\n",
    "    base_timestamps[2],\n",
    "]\n",
    "\n",
    "# Create synthetic dataset with known properties\n",
    "data = {\n",
    "    # Entity identifier and temporal index\n",
    "    'id': [1, 1, 1, 2, 2],\n",
    "    'timestamp': timestamps,\n",
    "    \n",
    "    # Numeric features for validation\n",
    "    'feature1': [1.0, 2.0, None, 4.0, 5.0],     # Float with missing values\n",
    "    'feature2': [1.5, 2.5, 3.5, None, 5.5],     # Float with missing values\n",
    "    'feature3': [10, 20, 30, 40, 50],           # Integer without missing values\n",
    "}\n",
    "\n",
    "@nw.narwhalify\n",
    "def validate_temporal_uniqueness(df: FrameT, id_col: str, time_col: str) -> FrameT:\n",
    "    \"\"\"Validate temporal uniqueness within entity groups.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : FrameT\n",
    "        Input DataFrame with time series data\n",
    "    id_col : str\n",
    "        Column name for entity identifier\n",
    "    time_col : str\n",
    "        Column name for temporal index\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    FrameT\n",
    "        DataFrame containing any duplicate timestamps found\n",
    "    \"\"\"\n",
    "    # Group by entity and timestamp - stays lazy for efficiency\n",
    "    counts = df.group_by([id_col, time_col]).agg([\n",
    "        nw.col(time_col).count().alias(\"count\")\n",
    "    ])\n",
    "    \n",
    "    # Filter for duplicates - still lazy until results needed\n",
    "    return counts.filter(nw.col(\"count\") > 1)\n",
    "\n",
    "# Initialize DataFrames for each backend\n",
    "df_pd = pd.DataFrame(data)           # Pandas backend\n",
    "df_pl = pl.DataFrame(data)          # Polars backend\n",
    "df_dask = dd.from_pandas(df_pd, npartitions=2)  # Dask backend\n",
    "\n",
    "# Test across backends\n",
    "print(\"Temporal Uniqueness Validation Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, df in [\n",
    "    (\"Pandas\", df_pd),\n",
    "    (\"Polars\", df_pl),\n",
    "    (\"Dask\", df_dask)\n",
    "]:\n",
    "    print(f\"\\n{name} Backend Results:\")\n",
    "    print(\"-\" * 30)\n",
    "    df_nw = nw.from_native(df)\n",
    "    result = validate_temporal_uniqueness(df_nw, \"id\", \"timestamp\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
