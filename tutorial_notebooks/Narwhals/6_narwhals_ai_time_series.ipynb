{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: Time Series Operations with Narwhals\n",
    "\n",
    "This notebook demonstrates essential patterns for working with time series data using Narwhals, focusing on:\n",
    "\n",
    "1. **Group-by Time Series Operations**\n",
    "   - Efficient grouping by ID columns\n",
    "   - Temporal aggregations within groups\n",
    "   - Mixed frequency handling\n",
    "\n",
    "2. **Time Series Validation**\n",
    "   - Temporal uniqueness checks\n",
    "   - Group-level validation\n",
    "   - Data quality assurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 1: Time Series Group-by Operations\n",
    "\n",
    "**NOTICE**: The following examples are provided AS-IS for academic purposes and are subject to the user's specific requirements and use cases. Always refer to the latest API documentation for production use.\n",
    "\n",
    "Common time series tasks require grouping by an ID column and performing temporal operations within each group. These patterns are essential for ML workflows in various domains.\n",
    "\n",
    "The following table shows key use cases where these patterns are critical:\n",
    "\n",
    "| Use Case | Description |\n",
    "|----------|-------------|\n",
    "| Healthcare Analytics | Patient vitals monitoring over time (patient_id), treatment response analysis, longitudinal health studies |\n",
    "| Quantitative Finance | Multi-asset portfolio analysis (ticker_id), market regime detection, cross-sectional momentum studies |\n",
    "| Signal Processing | Multi-sensor data fusion (sensor_id), anomaly detection across sensor networks, distributed system monitoring |\n",
    "\n",
    "Key Narwhals patterns for time series operations:\n",
    "\n",
    "| Operation | ✅ Good Pattern | ❌ Bad Pattern |\n",
    "|-----------|----------------|----------------|\n",
    "| Group By | `df.group_by(id_col).agg([nw.col(\"value\").mean()])` | `df.groupby(id_col).agg({\"value\": \"mean\"})` |\n",
    "| Rolling | `df.with_columns([nw.col(\"value\").rolling_mean(2)])` | `df[\"value\"].rolling(2).mean()` |\n",
    "| Time Sort | `df.sort([time_col])` | `df.sort_values(time_col)` |\n",
    "| Null Check | `nw.col(\"value\").is_null().sum()` | `df[\"value\"].isnull().sum()` |\n",
    "\n",
    "Implementation considerations for robust time series processing:\n",
    "\n",
    "| Consideration | Description |\n",
    "|---------------|-------------|\n",
    "| Lazy Evaluation | • Chain operations for optimization<br>• Let Narwhals handle backend-specific optimizations<br>• Avoid unnecessary materializations |\n",
    "| Temporal Ordering | • Ensure proper time-based sorting<br>• Handle mixed frequencies<br>• Maintain group boundaries |\n",
    "| Backend Compatibility | • Use elementary aggregations<br>• Follow Narwhals patterns for operations<br>• Let Narwhals handle backend-specific details |\n",
    "| Data Quality | • Validate temporal uniqueness<br>• Handle missing values<br>• Check frequency consistency |\n",
    "\n",
    "These patterns ensure robust time series processing across different ML scenarios and DataFrame backends. Note that some DataFrame implementations like Dask have specific requirements (e.g., known divisions for rolling operations) due to their distributed nature. While Narwhals handles these backend-specific details, always consult the latest API documentation if you need to customize error handling or implement special cases.\n",
    "\n",
    "**Best Practices Summary:**\n",
    "1. ✅ Use elementary operations that work across all backends\n",
    "2. ✅ Let Narwhals handle backend-specific optimizations\n",
    "3. ✅ Pre-compute complex operations before grouping\n",
    "4. ✅ Document backend-specific requirements in docstrings\n",
    "5. ❌ Don't try to handle backend-specific details in your code\n",
    "6. ❌ Don't chain operations after group-by\n",
    "7. ❌ Don't use backend-specific operations\n",
    "\n",
    "The recommended approach is to use Narwhals patterns and let it manage backend-specific optimizations and limitations. This ensures your code remains maintainable and works consistently across different DataFrame implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing compute_group_metrics:\n",
      "   id start_time   end_time  avg_value  total_records  missing_records\n",
      "0   1 2023-01-01 2023-01-02       15.0              2                0\n",
      "1   2 2023-01-03 2023-01-04       30.0              1                1\n",
      "2   3 2023-01-05 2023-01-05       50.0              1                0\n",
      "shape: (3, 6)\n",
      "┌─────┬─────────────────────┬─────────────────────┬───────────┬───────────────┬─────────────────┐\n",
      "│ id  ┆ start_time          ┆ end_time            ┆ avg_value ┆ total_records ┆ missing_records │\n",
      "│ --- ┆ ---                 ┆ ---                 ┆ ---       ┆ ---           ┆ ---             │\n",
      "│ i64 ┆ datetime[ns]        ┆ datetime[ns]        ┆ f64       ┆ u32           ┆ u32             │\n",
      "╞═════╪═════════════════════╪═════════════════════╪═══════════╪═══════════════╪═════════════════╡\n",
      "│ 2   ┆ 2023-01-03 00:00:00 ┆ 2023-01-04 00:00:00 ┆ 30.0      ┆ 1             ┆ 1               │\n",
      "│ 1   ┆ 2023-01-01 00:00:00 ┆ 2023-01-02 00:00:00 ┆ 15.0      ┆ 2             ┆ 0               │\n",
      "│ 3   ┆ 2023-01-05 00:00:00 ┆ 2023-01-05 00:00:00 ┆ 50.0      ┆ 1             ┆ 0               │\n",
      "└─────┴─────────────────────┴─────────────────────┴───────────┴───────────────┴─────────────────┘\n",
      "Dask DataFrame Structure:\n",
      "                  id      start_time        end_time avg_value total_records missing_records\n",
      "npartitions=1                                                                               \n",
      "               int64  datetime64[ns]  datetime64[ns]   float64         int64           int64\n",
      "                 ...             ...             ...       ...           ...             ...\n",
      "Dask Name: reset_index, 10 expressions\n",
      "Expr=ResetIndex(frame=ColumnsSetter(frame=(GroupbyAggregation(frame=Assign(frame=df), arg=defaultdict(<class 'list'>, {'timestamp': ['min', 'max'], 'value': ['mean', 'count'], 'is_null': ['sum']}), observed=True, dropna=False))[[('timestamp', 'min'), ('timestamp', 'max'), ('value', 'mean'), ('value', 'count'), ('is_null', 'sum')]], columns=('start_time', 'end_time', 'avg_value', 'total_records', 'missing_records')))\n",
      "\n",
      "Testing compute_rolling_stats:\n",
      "   id  rolling_mean_2\n",
      "0   1            15.0\n",
      "1   2            25.0\n",
      "2   3             NaN\n",
      "shape: (3, 2)\n",
      "┌─────┬────────────────┐\n",
      "│ id  ┆ rolling_mean_2 │\n",
      "│ --- ┆ ---            │\n",
      "│ i64 ┆ f64            │\n",
      "╞═════╪════════════════╡\n",
      "│ 1   ┆ 15.0           │\n",
      "│ 2   ┆ 25.0           │\n",
      "│ 3   ┆ null           │\n",
      "└─────┴────────────────┘\n",
      "\n",
      "Dask rolling operations require known divisions:\n",
      "Can only rolling dataframes with known divisions\n",
      "See https://docs.dask.org/en/latest/dataframe-design.html#partitions\n",
      "for more information.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import narwhals as nw\n",
    "from narwhals.typing import FrameT\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import dask.dataframe as dd\n",
    "from typing import Dict, List, Optional, Union, Any, Literal\n",
    "\n",
    "\n",
    "@nw.narwhalify\n",
    "def compute_group_metrics(df: FrameT, id_col: str, time_col: str, value_col: str) -> FrameT:\n",
    "    \"\"\"Compute time series metrics by group.\n",
    "\n",
    "    This function demonstrates proper Narwhals patterns for universal backend support:\n",
    "    1. Uses only elementary aggregations (min, max, mean, count, sum)\n",
    "    2. Works consistently across Pandas, Polars, and Dask\n",
    "    3. Properly handles missing values and time ranges\n",
    "\n",
    "    Returns a DataFrame with metrics per group:\n",
    "    - start_time: First timestamp in group\n",
    "    - end_time: Last timestamp in group\n",
    "    - avg_value: Mean value in group\n",
    "    - total_records: Count of records\n",
    "    - missing_records: Count of nulls\n",
    "    \"\"\"\n",
    "    # Pre-compute null indicators\n",
    "    df_prep = df.with_columns([nw.col(value_col).is_null().alias(\"is_null\")])\n",
    "\n",
    "    # Use elementary aggregations\n",
    "    return df_prep.group_by(id_col).agg(\n",
    "        [\n",
    "            # Time range metrics (elementary)\n",
    "            nw.col(time_col).min().alias(\"start_time\"),\n",
    "            nw.col(time_col).max().alias(\"end_time\"),\n",
    "            # Value statistics (elementary)\n",
    "            nw.col(value_col).mean().alias(\"avg_value\"),\n",
    "            # Count metrics (elementary)\n",
    "            nw.col(value_col).count().alias(\"total_records\"),\n",
    "            nw.col(\"is_null\").sum().alias(\"missing_records\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "@nw.narwhalify\n",
    "def compute_rolling_stats(df: FrameT, id_col: str, time_col: str, value_col: str, window: int) -> FrameT:\n",
    "    \"\"\"Compute rolling statistics within groups.\n",
    "\n",
    "    This function demonstrates proper handling of complex operations:\n",
    "    1. Pre-computes rolling means before grouping\n",
    "    2. Uses elementary aggregations for group operations\n",
    "    3. Lets Narwhals handle backend-specific details\n",
    "\n",
    "    Note: Rolling operations have backend-specific requirements:\n",
    "    - Works with Pandas and Polars out of the box\n",
    "    - Dask requires known divisions (see Dask documentation)\n",
    "    - Let Narwhals handle these requirements through its error handling\n",
    "\n",
    "    Returns a DataFrame with rolling metrics per group:\n",
    "    - rolling_mean_{window}: Mean of rolling window values\n",
    "    \"\"\"\n",
    "    # First sort and compute rolling means\n",
    "    df_prep = df.sort([id_col, time_col]).with_columns([nw.col(value_col).rolling_mean(window).alias(\"rolling_value\")])\n",
    "\n",
    "    # Then do elementary group-by aggregation\n",
    "    return df_prep.group_by(id_col).agg([nw.col(\"rolling_value\").mean().alias(f\"rolling_mean_{window}\")])\n",
    "\n",
    "\n",
    "# Test data\n",
    "test_data = {\n",
    "    \"id\": [1, 1, 2, 2, 3],\n",
    "    \"timestamp\": pd.date_range(\"2023-01-01\", periods=5, freq=\"D\"),\n",
    "    \"value\": [10.0, 20.0, 30.0, None, 50.0],\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "df_pd = pd.DataFrame(test_data)\n",
    "df_pl = pl.DataFrame(test_data)\n",
    "df_dd = dd.from_pandas(df_pd, npartitions=2)\n",
    "\n",
    "# Test functions\n",
    "print(\"Testing compute_group_metrics:\")\n",
    "print(compute_group_metrics(nw.from_native(df_pd), \"id\", \"timestamp\", \"value\"))\n",
    "print(compute_group_metrics(nw.from_native(df_pl), \"id\", \"timestamp\", \"value\"))\n",
    "print(compute_group_metrics(nw.from_native(df_dd), \"id\", \"timestamp\", \"value\"))\n",
    "\n",
    "print(\"\\nTesting compute_rolling_stats:\")\n",
    "window = 2\n",
    "print(compute_rolling_stats(nw.from_native(df_pd), \"id\", \"timestamp\", \"value\", window))\n",
    "print(compute_rolling_stats(nw.from_native(df_pl), \"id\", \"timestamp\", \"value\", window))\n",
    "try:\n",
    "    print(compute_rolling_stats(nw.from_native(df_dd), \"id\", \"timestamp\", \"value\", window))\n",
    "except ValueError as e:\n",
    "    print(f\"\\nDask rolling operations require known divisions:\\n{str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 2: Time Series Validation\n",
    "\n",
    "Time series data often requires validation to ensure quality and consistency. Here are key validation patterns us\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing temporal uniqueness validation:\n",
      "--------------------------------------------------\n",
      "\n",
      "Pandas Result:\n",
      "   id  timestamp  count\n",
      "0   1 2023-01-01      2\n",
      "\n",
      "Polars Result:\n",
      "shape: (1, 3)\n",
      "┌─────┬─────────────────────┬───────┐\n",
      "│ id  ┆ timestamp           ┆ count │\n",
      "│ --- ┆ ---                 ┆ ---   │\n",
      "│ i64 ┆ datetime[μs]        ┆ u32   │\n",
      "╞═════╪═════════════════════╪═══════╡\n",
      "│ 1   ┆ 2023-01-01 00:00:00 ┆ 2     │\n",
      "└─────┴─────────────────────┴───────┘\n",
      "\n",
      "Dask Result:\n",
      "   id  timestamp  count\n",
      "0   1 2023-01-01      2\n",
      "\n",
      "Testing uniform frequency validation:\n",
      "--------------------------------------------------\n",
      "\n",
      "Pandas Result:\n",
      "   id start_time   end_time  points\n",
      "0   1 2023-01-01 2023-01-02       3\n",
      "1   2 2023-01-01 2023-01-04       3\n",
      "\n",
      "Polars Result:\n",
      "shape: (2, 4)\n",
      "┌─────┬─────────────────────┬─────────────────────┬────────┐\n",
      "│ id  ┆ start_time          ┆ end_time            ┆ points │\n",
      "│ --- ┆ ---                 ┆ ---                 ┆ ---    │\n",
      "│ i64 ┆ datetime[μs]        ┆ datetime[μs]        ┆ u32    │\n",
      "╞═════╪═════════════════════╪═════════════════════╪════════╡\n",
      "│ 1   ┆ 2023-01-01 00:00:00 ┆ 2023-01-02 00:00:00 ┆ 3      │\n",
      "│ 2   ┆ 2023-01-01 00:00:00 ┆ 2023-01-04 00:00:00 ┆ 3      │\n",
      "└─────┴─────────────────────┴─────────────────────┴────────┘\n",
      "\n",
      "Dask Result:\n",
      "   id start_time   end_time  points\n",
      "0   1 2023-01-01 2023-01-02       3\n",
      "1   2 2023-01-01 2023-01-04       3\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import narwhals as nw\n",
    "from narwhals.typing import FrameT\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import dask.dataframe as dd\n",
    "from typing import Dict, List, Optional, Union, Any, Literal\n",
    "\n",
    "\n",
    "@nw.narwhalify\n",
    "def validate_temporal_uniqueness(df: FrameT, id_col: str, time_col: str) -> FrameT:\n",
    "    \"\"\"Validate and report temporal uniqueness.\n",
    "\n",
    "    This demonstrates lazy evaluation for validation:\n",
    "    - Group and count operations can be optimized\n",
    "    - Return DataFrame for further processing\n",
    "    - Let caller decide when to materialize\n",
    "    \"\"\"\n",
    "    # Stage 1: Group by ID and time\n",
    "    counts = df.group_by([id_col, time_col]).agg([nw.col(time_col).count().alias(\"count\")])\n",
    "\n",
    "    # Stage 2: Filter for duplicates\n",
    "    return counts.filter(nw.col(\"count\") > 1)\n",
    "\n",
    "\n",
    "@nw.narwhalify\n",
    "def validate_uniform_frequency(df: FrameT, id_col: str, time_col: str) -> FrameT:\n",
    "    \"\"\"Validate time frequency consistency.\n",
    "\n",
    "    This demonstrates lazy evaluation for validation:\n",
    "    - Group and compute time spans\n",
    "    - Return DataFrame for further processing\n",
    "    - Let caller decide when to materialize\n",
    "    \"\"\"\n",
    "    # Stage 1: Group by ID and compute time spans\n",
    "    return df.group_by(id_col).agg(\n",
    "        [\n",
    "            nw.col(time_col).min().alias(\"start_time\"),\n",
    "            nw.col(time_col).max().alias(\"end_time\"),\n",
    "            nw.col(time_col).count().alias(\"points\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# Test data\n",
    "test_data = {\n",
    "    \"id\": [1, 1, 1, 2, 2, 2],\n",
    "    \"timestamp\": [\n",
    "        pd.Timestamp(\"2023-01-01\"),\n",
    "        pd.Timestamp(\"2023-01-01\"),  # Duplicate\n",
    "        pd.Timestamp(\"2023-01-02\"),\n",
    "        pd.Timestamp(\"2023-01-01\"),\n",
    "        pd.Timestamp(\"2023-01-02\"),\n",
    "        pd.Timestamp(\"2023-01-04\"),  # Non-uniform gap\n",
    "    ],\n",
    "    \"value\": [10.0, 20.0, 30.0, 40.0, 50.0, 60.0],\n",
    "}\n",
    "\n",
    "# Test with different backends\n",
    "print(\"Testing temporal uniqueness validation:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for backend, df in [\n",
    "    (\"Pandas\", pd.DataFrame(test_data)),\n",
    "    (\"Polars\", pl.DataFrame(test_data)),\n",
    "    (\"Dask\", dd.from_pandas(pd.DataFrame(test_data), npartitions=2)),\n",
    "]:\n",
    "    print(f\"\\n{backend} Result:\")\n",
    "    try:\n",
    "        result = validate_temporal_uniqueness(nw.from_native(df), \"id\", \"timestamp\")\n",
    "        if hasattr(result, \"compute\"):\n",
    "            result = result.compute()\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "print(\"\\nTesting uniform frequency validation:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for backend, df in [\n",
    "    (\"Pandas\", pd.DataFrame(test_data)),\n",
    "    (\"Polars\", pl.DataFrame(test_data)),\n",
    "    (\"Dask\", dd.from_pandas(pd.DataFrame(test_data), npartitions=2)),\n",
    "]:\n",
    "    print(f\"\\n{backend} Result:\")\n",
    "    try:\n",
    "        result = validate_uniform_frequency(nw.from_native(df), \"id\", \"timestamp\")\n",
    "        if hasattr(result, \"compute\"):\n",
    "            result = result.compute()\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways - Time Series Validation Patterns\n",
    "\n",
    "This example demonstrates robust patterns for validating time series data across different DataFrame backends.\n",
    "\n",
    "### Key Patterns\n",
    "\n",
    "1. **Keep Operations Lazy**\n",
    "   - ✅ Return DataFrames for further processing\n",
    "   - ✅ Use elementary operations (group_by, agg)\n",
    "   - ✅ Let caller decide when to materialize\n",
    "   - ❌ Don't extract scalars in validation functions\n",
    "   - ❌ Don't use over() for window operations\n",
    "\n",
    "2. **Backend Compatibility**\n",
    "   - ✅ Use group_by and agg for aggregations\n",
    "   - ✅ Handle compute() at caller level\n",
    "   - ✅ Use only Narwhals operations\n",
    "   - ❌ Don't use backend-specific operations\n",
    "   - ❌ Don't assume eager evaluation\n",
    "\n",
    "3. **Validation Results**\n",
    "   - ✅ Return DataFrames with validation details\n",
    "   - ✅ Include all relevant information\n",
    "   - ✅ Allow further processing if needed\n",
    "   - ❌ Don't force immediate materialization\n",
    "   - ❌ Don't return only boolean results\n",
    "\n",
    "### Example Results\n",
    "\n",
    "1. **Temporal Uniqueness**\n",
    "```\n",
    "   id  timestamp  count\n",
    "0   1 2023-01-01      2  # Shows duplicate timestamps\n",
    "```\n",
    "\n",
    "2. **Frequency Validation**\n",
    "```\n",
    "   id start_time   end_time  points\n",
    "0   1 2023-01-01 2023-01-02      3  # Regular frequency\n",
    "1   2 2023-01-01 2023-01-04      3  # Irregular frequency\n",
    "```\n",
    "\n",
    "### Why This Pattern Works\n",
    "\n",
    "1. **Universal Backend Support**\n",
    "   - Works with Pandas, Polars, and Dask\n",
    "   - No backend-specific code needed\n",
    "   - Consistent results across backends\n",
    "\n",
    "2. **Efficient Processing**\n",
    "   - Lazy evaluation enables optimization\n",
    "   - No unnecessary materializations\n",
    "   - Chainable with other operations\n",
    "\n",
    "3. **Rich Validation Results**\n",
    "   - Full details for analysis\n",
    "   - Supports further processing\n",
    "   - Clear validation outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 3: Mixed Frequency Handling\n",
    "\n",
    "Time series data often has mixed frequencies that need special handling:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
