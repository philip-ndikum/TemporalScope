{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TemporalScope Tutorial: Backend-Agnostic Functions Using Narwhals\n",
    "\n",
    "### Purpose\n",
    "This tutorial demonstrates how **TemporalScope** can leverage **Narwhals** to support backend-agnostic data operations. By building backend-agnostic functions, TemporalScope enables compatibility across multiple popular data processing libraries including **Pandas**, **Modin**, **Polars**, and **PyArrow**.\n",
    "\n",
    "### Key Steps\n",
    "\n",
    "1. **Create a Sample Dataset**: We begin by creating a synthetic dataset in Pandas, which we will transform and test for compatibility across Modin, Polars, and PyArrow.\n",
    "2. **Implement a Narwhals-Decorated Function**: With Narwhals’ `@narwhalify` decorator, we can create a backend-agnostic function that performs simple operations like aggregation or column transformations without needing to rewrite the logic for each backend.\n",
    "3. **Run Compatibility Tests**: Finally, we test the function across all supported backends to verify smooth execution across Pandas, Modin, Polars, and PyArrow.\n",
    "\n",
    "### Supported TemporalScope Backends\n",
    "The TemporalScope core API is designed to be compatible with a wide range of popular DataFrame backends. Here are the currently supported backends:\n",
    "\n",
    "- **Pandas**: General-purpose data processing in Python, compatible with Narwhals.\n",
    "- **Modin**: Parallelized Pandas-like library for distributed data processing.\n",
    "- **Polars**: Rust-based, highly efficient DataFrame library for analytics.\n",
    "- **PyArrow**: Apache Arrow-based DataFrame supporting large, in-memory data processing.\n",
    "- **Dask**: Distributed DataFrame library for parallel computation on large datasets.\n",
    "\n",
    "These backends allow TemporalScope users to scale or optimize their workflows without modifying code.\n",
    "\n",
    "### Advantages of Using Narwhals\n",
    "\n",
    "- **Uniform API**: The `@narwhalify` decorator from Narwhals creates a seamless backend-neutral execution environment, allowing developers to use the same function across multiple DataFrame libraries.\n",
    "- **Enhanced Compatibility**: Narwhals optimizes how data is accessed and manipulated across backends, ensuring the syntax and functions used are supported by each compatible backend.\n",
    "- **Simplified Codebase**: By using Narwhals for backend-agnostic functions, TemporalScope’s core logic can remain generalized, reducing code duplication and maintenance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 0: Backend Compatibility with TemporalScope and Narwhals\n",
    "\n",
    "This example demonstrates a basic setup to ensure compatibility across various DataFrame backends using TemporalScope’s backend utilities and Narwhals.\n",
    "\n",
    "1. **Check Supported Backends**:\n",
    "   - Retrieve and validate the supported backends with `get_temporalscope_backends()` and `is_valid_temporal_backend()` for `pandas`, `modin`, `polars`, and `pyarrow`.\n",
    "2. **Run Narwhals-Compatible Operation**:\n",
    "   - Define a backend-agnostic function with `@narwhalify` to aggregate basic column statistics, showcasing Narwhals’ compatibility layer across supported backends.\n",
    "3. **Compare Results Across Backends**:\n",
    "- Execute the Narwhals-compatible function on DataFrames from different backends and compare the results to ensure consistency across implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame Type: <class 'pandas.core.frame.DataFrame'>\n",
      "Result of Narwhals operation:\n",
      "    feature_1_sum  feature_2_mean\n",
      "0      96.801247        0.067414\n",
      "\n",
      "== TemporalScope and Narwhals Backend Checks ==\n",
      "Supported TemporalScope Backends: ['pandas', 'modin', 'pyarrow', 'polars', 'dask']\n",
      "Supported Narwhals Backends: ['pandas', 'modin', 'cudf', 'pyarrow', 'polars', 'dask', 'unknown']\n",
      "Pandas backend validation: FAILED name 'is_valid_temporal_backend' is not defined\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import narwhals as nw\n",
    "from temporalscope.core.core_utils import (\n",
    "    get_temporalscope_backends,\n",
    "    get_narwhals_backends,\n",
    ")\n",
    "from narwhals.typing import FrameT\n",
    "\n",
    "# Constants\n",
    "NUM_ROWS = 200\n",
    "SEED = 42\n",
    "\n",
    "# Step 1: Generate sample time-series data in Pandas\n",
    "np.random.seed(SEED)\n",
    "date_range = pd.date_range(start=\"2023-01-01\", periods=NUM_ROWS, freq=\"D\")\n",
    "data = pd.DataFrame({\n",
    "    \"datetime\": date_range,\n",
    "    \"feature_1\": np.random.rand(NUM_ROWS),\n",
    "    \"feature_2\": np.random.randn(NUM_ROWS)\n",
    "})\n",
    "\n",
    "# Step 2: Define a Narwhals-compatible function using @narwhalify\n",
    "@nw.narwhalify\n",
    "def test_narwhal_conversion(df: FrameT) -> FrameT:\n",
    "    \"\"\"Perform Narwhals operations on a compatible DataFrame.\"\"\"\n",
    "    return df.select(\n",
    "        feature_1_sum=nw.col(\"feature_1\").sum(),\n",
    "        feature_2_mean=nw.col(\"feature_2\").mean()\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the Narwhals operation\n",
    "    print(\"Original DataFrame Type:\", type(data))\n",
    "    result_df = test_narwhal_conversion(data)\n",
    "    print(\"Result of Narwhals operation:\\n\", result_df)\n",
    "    \n",
    "    # Step 3: Check backend support using TemporalScope core_utils\n",
    "    print(\"\\n== TemporalScope and Narwhals Backend Checks ==\")\n",
    "    temporalscope_backends = get_temporalscope_backends()\n",
    "    print(\"Supported TemporalScope Backends:\", temporalscope_backends)\n",
    "\n",
    "    narwhals_backends = get_narwhals_backends()\n",
    "    print(\"Supported Narwhals Backends:\", narwhals_backends)\n",
    "\n",
    "    # Validate if 'pandas' backend is supported\n",
    "    try:\n",
    "        is_valid_temporal_backend('pandas')\n",
    "        print(\"Pandas backend validation: PASSED\")\n",
    "    except Exception as e:\n",
    "        print(\"Pandas backend validation: FAILED\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Pandas Backend ===\n",
      "Original DataFrame Backend: pandas\n",
      "Is 'datetime' timestamp-like? True\n",
      "\n",
      "Result of Narwhals operation:\n",
      "    feature_1_sum  feature_2_mean\n",
      "0      47.018074        -0.00108\n",
      "\n",
      "=== Testing Polars Backend ===\n",
      "Original DataFrame Backend: polars\n",
      "Is 'datetime' timestamp-like? True\n",
      "\n",
      "Result of Narwhals operation:\n",
      " shape: (1, 2)\n",
      "┌───────────────┬────────────────┐\n",
      "│ feature_1_sum ┆ feature_2_mean │\n",
      "│ ---           ┆ ---            │\n",
      "│ f64           ┆ f64            │\n",
      "╞═══════════════╪════════════════╡\n",
      "│ 47.018074     ┆ -0.00108       │\n",
      "└───────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import narwhals as nw\n",
    "from narwhals.typing import FrameT\n",
    "\n",
    "# Constants\n",
    "NUM_ROWS = 100\n",
    "SEED = 42\n",
    "\n",
    "# Step 1: Generate sample time-series data in Pandas\n",
    "np.random.seed(SEED)\n",
    "date_range = pd.date_range(start=\"2023-01-01\", periods=NUM_ROWS, freq=\"D\")\n",
    "data_pandas = pd.DataFrame({\n",
    "    \"datetime\": date_range,\n",
    "    \"feature_1\": np.random.rand(NUM_ROWS),\n",
    "    \"feature_2\": np.random.randn(NUM_ROWS)\n",
    "})\n",
    "\n",
    "# Convert Pandas DataFrame to Polars\n",
    "data_polars = pl.DataFrame(data_pandas)\n",
    "\n",
    "# Utility Functions\n",
    "def is_timestamp_column(df: FrameT, col: str) -> bool:\n",
    "    \"\"\"Check if a column is timestamp-like.\"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' does not exist in the DataFrame.\")\n",
    "    \n",
    "    try:\n",
    "        if isinstance(df, pd.DataFrame):  # Pandas check\n",
    "            return pd.api.types.is_datetime64_any_dtype(df[col])\n",
    "        elif isinstance(df, pl.DataFrame):  # Polars check\n",
    "            return str(df.schema[col]).startswith(\"Datetime\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported DataFrame type: {type(df)}\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error while checking column type: {e}\")\n",
    "\n",
    "def get_backend_info(df: FrameT) -> str:\n",
    "    \"\"\"Determine the original backend of the DataFrame.\"\"\"\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        return \"pandas\"\n",
    "    elif isinstance(df, pl.DataFrame):\n",
    "        return \"polars\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Narwhals Function\n",
    "@nw.narwhalify\n",
    "def test_narwhal_conversion(df: FrameT) -> FrameT:\n",
    "    \"\"\"Perform Narwhals operations on a compatible DataFrame.\"\"\"\n",
    "    return df.select(\n",
    "        feature_1_sum=nw.col(\"feature_1\").sum(),\n",
    "        feature_2_mean=nw.col(\"feature_2\").mean()\n",
    "    )\n",
    "\n",
    "# Main Test Logic\n",
    "def run_tests(data: FrameT, backend_name: str):\n",
    "    print(f\"\\n=== Testing {backend_name} Backend ===\")\n",
    "    \n",
    "    # Original Backend\n",
    "    try:\n",
    "        original_backend = get_backend_info(data)\n",
    "        print(\"Original DataFrame Backend:\", original_backend)\n",
    "    except Exception as e:\n",
    "        print(\"Error determining backend:\", e)\n",
    "\n",
    "    # Timestamp Check\n",
    "    try:\n",
    "        is_timestamp = is_timestamp_column(data, \"datetime\")\n",
    "        print(\"Is 'datetime' timestamp-like?\", is_timestamp)\n",
    "    except Exception as e:\n",
    "        print(\"Timestamp check failed:\", e)\n",
    "\n",
    "    # Narwhals Operation\n",
    "    try:\n",
    "        result_df = test_narwhal_conversion(data)\n",
    "        print(\"\\nResult of Narwhals operation:\\n\", result_df)\n",
    "    except Exception as e:\n",
    "        print(\"Narwhals operation failed:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with Pandas\n",
    "    run_tests(data_pandas, \"Pandas\")\n",
    "\n",
    "    # Test with Polars\n",
    "    run_tests(data_polars, \"Polars\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Using Narwhals for Backend-Agnostic Null Check\n",
    "\n",
    "Narwhals enables robust, backend-agnostic data processing, which can streamline workflows across Pandas, Modin, Polars, and PyArrow backends. Here’s how to create a simple backend-agnostic null-check function and test it across multiple frameworks.\n",
    "\n",
    "1. **Create a Synthetic DataFrame**:\n",
    "   - Use a function like `generate_data_time_series()` to create a manageable dataset, with defaults that allow flexibility for larger data sizes.\n",
    "\n",
    "2. **Define a Narwhals-Compatible Function**:\n",
    "   - Use the `@narwhalify` decorator to convert standard DataFrame operations to backend-agnostic ones.\n",
    "   - For example, `check_nulls_nw()` checks for null values without being tied to a specific backend.\n",
    "\n",
    "3. **Test Across Multiple Backends**:\n",
    "   - Convert the DataFrame to various backends and execute the Narwhals function to verify compatibility and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas -> Executed Successfully, Has Nulls:    has_nulls\n",
      "0       True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The size of /dev/shm is too small (3904958464 bytes). The required size at least half of RAM (4017184768 bytes). Please, delete files in /dev/shm or increase size of /dev/shm with --shm-size in Docker. Also, you can can override the memory size for each Ray worker (in bytes) to the MODIN_MEMORY environment variable.\n",
      "2024-11-11 11:34:11,220\tINFO worker.py:1816 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modin -> Executed Successfully, Has Nulls:    has_nulls\n",
      "0       True\n",
      "\n",
      "pyarrow -> Executed Successfully, Has Nulls: pyarrow.Table\n",
      "has_nulls: bool\n",
      "----\n",
      "has_nulls: [[true]]\n",
      "\n",
      "polars -> Executed Successfully, Has Nulls: shape: (1, 1)\n",
      "┌───────────┐\n",
      "│ has_nulls │\n",
      "│ ---       │\n",
      "│ bool      │\n",
      "╞═══════════╡\n",
      "│ true      │\n",
      "└───────────┘\n",
      "\n",
      "dask -> Executed Successfully, Has Nulls: Dask DataFrame Structure:\n",
      "              has_nulls\n",
      "npartitions=1          \n",
      "                   bool\n",
      "                    ...\n",
      "Dask Name: to_frame, 11 expressions\n",
      "Expr=ToFrame(frame=RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=IsNa(frame=df['feature_1']), index='feature_1')).any()), index='feature_1'))[0]), index='has_nulls'))\n",
      "\n",
      "\n",
      "--- Summary of Backend Compatibility ---\n",
      "{'Backend': 'pandas', 'Has Nulls':    has_nulls\n",
      "0       True, 'Executed Successfully': True}\n",
      "{'Backend': 'modin', 'Has Nulls':    has_nulls\n",
      "0       True, 'Executed Successfully': True}\n",
      "{'Backend': 'pyarrow', 'Has Nulls': pyarrow.Table\n",
      "has_nulls: bool\n",
      "----\n",
      "has_nulls: [[true]], 'Executed Successfully': True}\n",
      "{'Backend': 'polars', 'Has Nulls': shape: (1, 1)\n",
      "┌───────────┐\n",
      "│ has_nulls │\n",
      "│ ---       │\n",
      "│ bool      │\n",
      "╞═══════════╡\n",
      "│ true      │\n",
      "└───────────┘, 'Executed Successfully': True}\n",
      "{'Backend': 'dask', 'Has Nulls': Dask DataFrame Structure:\n",
      "              has_nulls\n",
      "npartitions=1          \n",
      "                   bool\n",
      "                    ...\n",
      "Dask Name: to_frame, 11 expressions\n",
      "Expr=ToFrame(frame=RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=IsNa(frame=df['feature_1']), index='feature_1')).any()), index='feature_1'))[0]), index='has_nulls')), 'Executed Successfully': True}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import modin.pandas as mpd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import dask.dataframe as dd\n",
    "import narwhals as nw\n",
    "import numpy as np\n",
    "from narwhals.typing import FrameT\n",
    "from temporalscope.core.core_utils import TEMPORALSCOPE_CORE_BACKEND_TYPES, SupportedTemporalDataFrame\n",
    "\n",
    "NUM_ROWS = 100\n",
    "SEED = 42\n",
    "\n",
    "def generate_data(num_rows: int = NUM_ROWS, backend: str = \"pandas\") -> SupportedTemporalDataFrame:\n",
    "    \"\"\"Generates a time-series DataFrame with specified backend, adding features and a target.\"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    data = pd.DataFrame({\n",
    "        \"datetime\": pd.date_range(start=\"2023-01-01\", periods=num_rows, freq=\"D\")\n",
    "    })\n",
    "    \n",
    "    # Generate 5 features\n",
    "    for i in range(5):\n",
    "        data[f\"feature_{i+1}\"] = np.random.randn(num_rows)\n",
    "    data[\"target\"] = np.random.rand(num_rows)\n",
    "    data.loc[0, 'feature_1'] = None  # Inject a null value for testing\n",
    "\n",
    "    # Convert to specified backend\n",
    "    if backend == \"modin\":\n",
    "        return mpd.DataFrame(data)\n",
    "    elif backend == \"polars\":\n",
    "        return pl.DataFrame(data)\n",
    "    elif backend == \"pyarrow\":\n",
    "        return pa.Table.from_pandas(data)\n",
    "    elif backend == \"dask\":\n",
    "        return dd.from_pandas(data, npartitions=2)\n",
    "    return data\n",
    "\n",
    "@nw.narwhalify\n",
    "def check_nulls_nw(df: FrameT) -> FrameT:\n",
    "    \"\"\"Checks for null values in 'feature_1' in a backend-agnostic way using Narwhals.\"\"\"\n",
    "    return df.select(\n",
    "        has_nulls=nw.col(\"feature_1\").is_null().any()\n",
    "    )\n",
    "\n",
    "def test_backends():\n",
    "    \"\"\"Tests `check_nulls_nw` on all supported TemporalScope backends.\"\"\"\n",
    "    results = []\n",
    "    for backend_name in TEMPORALSCOPE_CORE_BACKEND_TYPES.keys():\n",
    "        data_df = generate_data(backend=backend_name)\n",
    "        try:\n",
    "            has_nulls = check_nulls_nw(data_df)\n",
    "            results.append({\n",
    "                \"Backend\": backend_name,\n",
    "                \"Has Nulls\": has_nulls,\n",
    "                \"Executed Successfully\": True\n",
    "            })\n",
    "            print(f\"{backend_name} -> Executed Successfully, Has Nulls: {has_nulls}\\n\")\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"Backend\": backend_name,\n",
    "                \"Executed Successfully\": False,\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "            print(f\"{backend_name} -> Failed with error: {e}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    backend_results = test_backends()\n",
    "    print(\"\\n--- Summary of Backend Compatibility ---\")\n",
    "    for result in backend_results:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Calculating Summary Statistics in a Backend-Agnostic Manner\n",
    "\n",
    "This example demonstrates **Narwhals**' ability to calculate summary statistics (mean, sum, standard deviation) across TemporalScope’s supported backends—**Pandas**, **Modin**, **Polars**, **PyArrow**, and **Dask**—with a backend-agnostic function that leverages Narwhals’ compatibility layer. \n",
    "\n",
    "### Key Steps\n",
    "\n",
    "1. **Generate Synthetic Data Across Backends**:\n",
    "   - Using `generate_data()`, we create a synthetic time-series DataFrame with multiple features and a target column. This dataset is compatible with multiple backends and provides the basis for backend-agnostic operations.\n",
    "\n",
    "2. **Define a Narwhals-Compatible Summary Function**:\n",
    "   - `calculate_summaries_nw()` uses the `@narwhalify` decorator to compute mean, sum, and standard deviation for each feature in a backend-agnostic manner.\n",
    "\n",
    "3. **Understanding Lazy vs. Eager Evaluation**:\n",
    "   - **Lazy Evaluation** (Polars, Dask): Allows efficient, optimized computation by delaying operations until results are explicitly requested, like with `.collect()`. Narwhals supports lazy evaluation for backends like Polars, automatically handling eager execution where needed.\n",
    "   - **Eager Evaluation** (Pandas, Modin, PyArrow): Calculates results immediately, useful for smaller datasets or immediate result retrieval.\n",
    "   - Narwhals adapts between lazy and eager modes based on backend needs, ensuring that computations are handled correctly even across large and distributed datasets.\n",
    "\n",
    "4. **Test Across Multiple Backends**:\n",
    "   - The function is tested across the TemporalScope-supported backends, with each backend returning summary statistics in its native structure.\n",
    "   - Results vary slightly in format: Pandas and Modin return DataFrames, PyArrow returns a `Table`, Polars provides its own optimized `DataFrame` structure, and Dask shows results as a Dask DataFrame structure.\n",
    "\n",
    "This example showcases Narwhals’ seamless support for multi-backend compatibility, optimizing data operations across frameworks without modifying the core logic.\n",
    "~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas -> Executed Successfully, Summaries:\n",
      "   feature_1_mean  feature_1_sum  feature_1_std  feature_2_mean  \\\n",
      "0        0.471147      46.643534       0.298846        0.497832   \n",
      "\n",
      "   feature_2_sum  feature_2_std  feature_3_mean  feature_3_sum  feature_3_std  \\\n",
      "0      49.783172       0.293111        0.517601      51.760133       0.293426   \n",
      "\n",
      "   feature_4_mean  feature_4_sum  feature_4_std  feature_5_mean  \\\n",
      "0        0.491149      49.114894       0.293452        0.516046   \n",
      "\n",
      "   feature_5_sum  feature_5_std  \n",
      "0      51.604582       0.318601  \n",
      "\n",
      "modin -> Executed Successfully, Summaries:\n",
      "   feature_1_mean  feature_1_sum  feature_1_std  feature_2_mean  \\\n",
      "0        0.471147      46.643534       0.298846        0.497832   \n",
      "\n",
      "   feature_2_sum  feature_2_std  feature_3_mean  feature_3_sum  feature_3_std  \\\n",
      "0      49.783172       0.293111        0.517601      51.760133       0.293426   \n",
      "\n",
      "   feature_4_mean  feature_4_sum  feature_4_std  feature_5_mean  \\\n",
      "0        0.491149      49.114894       0.293452        0.516046   \n",
      "\n",
      "   feature_5_sum  feature_5_std  \n",
      "0      51.604582       0.318601  \n",
      "\n",
      "pyarrow -> Executed Successfully, Summaries:\n",
      "pyarrow.Table\n",
      "feature_1_mean: double\n",
      "feature_1_sum: double\n",
      "feature_1_std: double\n",
      "feature_2_mean: double\n",
      "feature_2_sum: double\n",
      "feature_2_std: double\n",
      "feature_3_mean: double\n",
      "feature_3_sum: double\n",
      "feature_3_std: double\n",
      "feature_4_mean: double\n",
      "feature_4_sum: double\n",
      "feature_4_std: double\n",
      "feature_5_mean: double\n",
      "feature_5_sum: double\n",
      "feature_5_std: double\n",
      "----\n",
      "feature_1_mean: [[0.4711468102926623]]\n",
      "feature_1_sum: [[46.64353421897357]]\n",
      "feature_1_std: [[0.29884566169994814]]\n",
      "feature_2_mean: [[0.4978317231550229]]\n",
      "feature_2_sum: [[49.78317231550229]]\n",
      "feature_2_std: [[0.2931112525150842]]\n",
      "feature_3_mean: [[0.5176013307472443]]\n",
      "feature_3_sum: [[51.760133074724436]]\n",
      "feature_3_std: [[0.29342624706090614]]\n",
      "feature_4_mean: [[0.49114894080503435]]\n",
      "...\n",
      "\n",
      "polars -> Executed Successfully, Summaries:\n",
      "shape: (1, 15)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ feature_1 ┆ feature_1 ┆ feature_1 ┆ feature_2 ┆ … ┆ feature_4 ┆ feature_5 ┆ feature_5 ┆ feature_ │\n",
      "│ _mean     ┆ _sum      ┆ _std      ┆ _mean     ┆   ┆ _std      ┆ _mean     ┆ _sum      ┆ 5_std    │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 0.471147  ┆ 46.643534 ┆ 0.298846  ┆ 0.497832  ┆ … ┆ 0.293452  ┆ 0.516046  ┆ 51.604582 ┆ 0.318601 │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "\n",
      "dask -> Executed Successfully, Summaries:\n",
      "Dask DataFrame Structure:\n",
      "              feature_1_mean feature_1_sum feature_1_std feature_2_mean feature_2_sum feature_2_std feature_3_mean feature_3_sum feature_3_std feature_4_mean feature_4_sum feature_4_std feature_5_mean feature_5_sum feature_5_std\n",
      "npartitions=1                                                                                                                                                                                                                       \n",
      "                     float64       float64       float64        float64       float64       float64        float64       float64       float64        float64       float64       float64        float64       float64       float64\n",
      "                         ...           ...           ...            ...           ...           ...            ...           ...           ...            ...           ...           ...            ...           ...           ...\n",
      "Dask Name: concat, 117 expressions\n",
      "Expr=Concat(frames=[RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_1'].mean()), index='feature_1'), index='feature_1_mean'))[0]), index='feature_1_mean'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_1'].sum()), index='feature_1'), index='feature_1_sum'))[0]), index='feature_1_sum'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=MapPartitions(sqrt)), index='feature_1'), index='feature_1_std'))[0]), index='feature_1_std'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_2'].mean()), index='feature_2'), index='feature_2_mean'))[0]), index='feature_2_mean'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_2'].sum()), index='feature_2'), index='feature_2_sum'))[0]), index='feature_2_sum'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=MapPartitions(sqrt)), index='feature_2'), index='feature_2_std'))[0]), index='feature_2_std'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_3'].mean()), index='feature_3'), index='feature_3_mean'))[0]), index='feature_3_mean'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_3'].sum()), index='feature_3'), index='feature_3_sum'))[0]), index='feature_3_sum'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=MapPartitions(sqrt)), index='feature_3'), index='feature_3_std'))[0]), index='feature_3_std'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_4'].mean()), index='feature_4'), index='feature_4_mean'))[0]), index='feature_4_mean'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_4'].sum()), index='feature_4'), index='feature_4_sum'))[0]), index='feature_4_sum'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=MapPartitions(sqrt)), index='feature_4'), index='feature_4_std'))[0]), index='feature_4_std'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_5'].mean()), index='feature_5'), index='feature_5_mean'))[0]), index='feature_5_mean'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_5'].sum()), index='feature_5'), index='feature_5_sum'))[0]), index='feature_5_sum'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=MapPartitions(sqrt)), index='feature_5'), index='feature_5_std'))[0]), index='feature_5_std')], axis=1)\n",
      "\n",
      "\n",
      "--- Summary of Backend Compatibility ---\n",
      "{'Backend': 'pandas', 'Summaries':    feature_1_mean  feature_1_sum  feature_1_std  feature_2_mean  \\\n",
      "0        0.471147      46.643534       0.298846        0.497832   \n",
      "\n",
      "   feature_2_sum  feature_2_std  feature_3_mean  feature_3_sum  feature_3_std  \\\n",
      "0      49.783172       0.293111        0.517601      51.760133       0.293426   \n",
      "\n",
      "   feature_4_mean  feature_4_sum  feature_4_std  feature_5_mean  \\\n",
      "0        0.491149      49.114894       0.293452        0.516046   \n",
      "\n",
      "   feature_5_sum  feature_5_std  \n",
      "0      51.604582       0.318601  , 'Executed Successfully': True}\n",
      "{'Backend': 'modin', 'Summaries':    feature_1_mean  feature_1_sum  feature_1_std  feature_2_mean  \\\n",
      "0        0.471147      46.643534       0.298846        0.497832   \n",
      "\n",
      "   feature_2_sum  feature_2_std  feature_3_mean  feature_3_sum  feature_3_std  \\\n",
      "0      49.783172       0.293111        0.517601      51.760133       0.293426   \n",
      "\n",
      "   feature_4_mean  feature_4_sum  feature_4_std  feature_5_mean  \\\n",
      "0        0.491149      49.114894       0.293452        0.516046   \n",
      "\n",
      "   feature_5_sum  feature_5_std  \n",
      "0      51.604582       0.318601  , 'Executed Successfully': True}\n",
      "{'Backend': 'pyarrow', 'Summaries': pyarrow.Table\n",
      "feature_1_mean: double\n",
      "feature_1_sum: double\n",
      "feature_1_std: double\n",
      "feature_2_mean: double\n",
      "feature_2_sum: double\n",
      "feature_2_std: double\n",
      "feature_3_mean: double\n",
      "feature_3_sum: double\n",
      "feature_3_std: double\n",
      "feature_4_mean: double\n",
      "feature_4_sum: double\n",
      "feature_4_std: double\n",
      "feature_5_mean: double\n",
      "feature_5_sum: double\n",
      "feature_5_std: double\n",
      "----\n",
      "feature_1_mean: [[0.4711468102926623]]\n",
      "feature_1_sum: [[46.64353421897357]]\n",
      "feature_1_std: [[0.29884566169994814]]\n",
      "feature_2_mean: [[0.4978317231550229]]\n",
      "feature_2_sum: [[49.78317231550229]]\n",
      "feature_2_std: [[0.2931112525150842]]\n",
      "feature_3_mean: [[0.5176013307472443]]\n",
      "feature_3_sum: [[51.760133074724436]]\n",
      "feature_3_std: [[0.29342624706090614]]\n",
      "feature_4_mean: [[0.49114894080503435]]\n",
      "..., 'Executed Successfully': True}\n",
      "{'Backend': 'polars', 'Summaries': shape: (1, 15)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ feature_1 ┆ feature_1 ┆ feature_1 ┆ feature_2 ┆ … ┆ feature_4 ┆ feature_5 ┆ feature_5 ┆ feature_ │\n",
      "│ _mean     ┆ _sum      ┆ _std      ┆ _mean     ┆   ┆ _std      ┆ _mean     ┆ _sum      ┆ 5_std    │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ f64       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 0.471147  ┆ 46.643534 ┆ 0.298846  ┆ 0.497832  ┆ … ┆ 0.293452  ┆ 0.516046  ┆ 51.604582 ┆ 0.318601 │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘, 'Executed Successfully': True}\n",
      "{'Backend': 'dask', 'Summaries': Dask DataFrame Structure:\n",
      "              feature_1_mean feature_1_sum feature_1_std feature_2_mean feature_2_sum feature_2_std feature_3_mean feature_3_sum feature_3_std feature_4_mean feature_4_sum feature_4_std feature_5_mean feature_5_sum feature_5_std\n",
      "npartitions=1                                                                                                                                                                                                                       \n",
      "                     float64       float64       float64        float64       float64       float64        float64       float64       float64        float64       float64       float64        float64       float64       float64\n",
      "                         ...           ...           ...            ...           ...           ...            ...           ...           ...            ...           ...           ...            ...           ...           ...\n",
      "Dask Name: concat, 117 expressions\n",
      "Expr=Concat(frames=[RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_1'].mean()), index='feature_1'), index='feature_1_mean'))[0]), index='feature_1_mean'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_1'].sum()), index='feature_1'), index='feature_1_sum'))[0]), index='feature_1_sum'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=MapPartitions(sqrt)), index='feature_1'), index='feature_1_std'))[0]), index='feature_1_std'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_2'].mean()), index='feature_2'), index='feature_2_mean'))[0]), index='feature_2_mean'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_2'].sum()), index='feature_2'), index='feature_2_sum'))[0]), index='feature_2_sum'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=MapPartitions(sqrt)), index='feature_2'), index='feature_2_std'))[0]), index='feature_2_std'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_3'].mean()), index='feature_3'), index='feature_3_mean'))[0]), index='feature_3_mean'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_3'].sum()), index='feature_3'), index='feature_3_sum'))[0]), index='feature_3_sum'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=MapPartitions(sqrt)), index='feature_3'), index='feature_3_std'))[0]), index='feature_3_std'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_4'].mean()), index='feature_4'), index='feature_4_mean'))[0]), index='feature_4_mean'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_4'].sum()), index='feature_4'), index='feature_4_sum'))[0]), index='feature_4_sum'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=MapPartitions(sqrt)), index='feature_4'), index='feature_4_std'))[0]), index='feature_4_std'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_5'].mean()), index='feature_5'), index='feature_5_mean'))[0]), index='feature_5_mean'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=df['feature_5'].sum()), index='feature_5'), index='feature_5_sum'))[0]), index='feature_5_sum'), RenameSeries(frame=ScalarToSeries(frame=(RenameSeries(frame=RenameSeries(frame=ScalarToSeries(frame=MapPartitions(sqrt)), index='feature_5'), index='feature_5_std'))[0]), index='feature_5_std')], axis=1), 'Executed Successfully': True}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import modin.pandas as mpd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import dask.dataframe as dd\n",
    "import narwhals as nw\n",
    "from narwhals.typing import FrameT\n",
    "import numpy as np\n",
    "from temporalscope.core.core_utils import TEMPORALSCOPE_CORE_BACKEND_TYPES\n",
    "\n",
    "# Constants\n",
    "NUM_ROWS = 100\n",
    "SEED = 42\n",
    "\n",
    "# Data generator function\n",
    "def generate_data(num_rows: int = NUM_ROWS, backend: str = \"pandas\") -> pd.DataFrame:\n",
    "    \"\"\"Generates a time-series DataFrame with multiple features and a target column.\"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    data = pd.DataFrame({\n",
    "        \"datetime\": pd.date_range(start=\"2023-01-01\", periods=num_rows, freq=\"D\"),\n",
    "        **{f\"feature_{i}\": np.random.rand(num_rows) for i in range(1, 6)},  # Generating 5 random features\n",
    "        \"target\": np.random.randint(0, 100, num_rows)  # Random integer target\n",
    "    })\n",
    "    data.loc[0, 'feature_1'] = None  # Injecting a null value for testing\n",
    "    \n",
    "    # Convert to the specified backend\n",
    "    if backend == \"modin\":\n",
    "        return mpd.DataFrame(data)\n",
    "    elif backend == \"polars\":\n",
    "        return pl.DataFrame(data)\n",
    "    elif backend == \"pyarrow\":\n",
    "        return pa.Table.from_pandas(data)\n",
    "    elif backend == \"dask\":\n",
    "        return dd.from_pandas(data, npartitions=2)\n",
    "    return data  # Default to Pandas DataFrame\n",
    "\n",
    "# Narwhals-compatible function for summary statistics\n",
    "@nw.narwhalify\n",
    "def calculate_summaries_nw(df: FrameT) -> FrameT:\n",
    "    \"\"\"Calculates mean, sum, and standard deviation for each feature using Narwhals.\"\"\"\n",
    "    expressions = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith(\"feature_\"):\n",
    "            expressions.extend([\n",
    "                nw.col(col).mean().alias(f\"{col}_mean\"),\n",
    "                nw.col(col).sum().alias(f\"{col}_sum\"),\n",
    "                nw.col(col).std().alias(f\"{col}_std\")\n",
    "            ])\n",
    "    return df.select(*expressions)\n",
    "\n",
    "# Testing function across all backends\n",
    "def test_backends():\n",
    "    \"\"\"Tests `calculate_summaries_nw` on all supported TemporalScope backends.\"\"\"\n",
    "    results = []\n",
    "    for backend_name in TEMPORALSCOPE_CORE_BACKEND_TYPES.keys():\n",
    "        data_df = generate_data(backend=backend_name)\n",
    "        try:\n",
    "            summaries = calculate_summaries_nw(data_df)\n",
    "            results.append({\n",
    "                \"Backend\": backend_name,\n",
    "                \"Summaries\": summaries,\n",
    "                \"Executed Successfully\": True\n",
    "            })\n",
    "            print(f\"{backend_name} -> Executed Successfully, Summaries:\\n{summaries}\\n\")\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"Backend\": backend_name,\n",
    "                \"Executed Successfully\": False,\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "            print(f\"{backend_name} -> Failed with error: {e}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the backend compatibility tests\n",
    "    backend_results = test_backends()\n",
    "    print(\"\\n--- Summary of Backend Compatibility ---\")\n",
    "    for result in backend_results:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Scaling and Lagging Features in a Backend-Agnostic Manner\n",
    "\n",
    "In this example, we demonstrate **Narwhals**’ capability to apply scaling and lag transformations across TemporalScope's supported backends. This approach is beneficial for time-series analysis, where lagging can reveal important sequential dependencies. Narwhals allows us to scale and lag feature columns consistently across multiple frameworks, maintaining a unified codebase.\n",
    "\n",
    "### Key Steps\n",
    "\n",
    "1. **Generate Synthetic Data Across Backends**:\n",
    "   - Using `generate_data()`, we create a synthetic DataFrame with multiple features and a target column, which can be applied to each backend for testing.\n",
    "\n",
    "2. **Define a Narwhals-Compatible Scaling and Lagging Function**:\n",
    "   - `scale_and_lag_features_nw()` applies scaling and lagging transformations to each feature column in a backend-agnostic manner. Scaling standardizes each feature, and lagging shifts values by a defined number of steps, revealing sequential dependencies.\n",
    "\n",
    "3. **Test Across Multiple Backends**:\n",
    "   - Run the function on each backend, returning results in the native format for each. This verifies compatibility and consistent application of transformations across Pandas, Modin, Polars, PyArrow, and Dask backends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas -> Executed Successfully, Transformed Data:\n",
      "     datetime  feature_1  feature_2  feature_3  feature_4  feature_5  target  \\\n",
      "0  2023-01-01   0.374540   0.031429   0.642032   0.051682   0.103124      62   \n",
      "1  2023-01-02   0.950714   0.636410   0.084140   0.531355   0.902553      16   \n",
      "2  2023-01-03   0.731994   0.314356   0.161629   0.540635   0.505252      72   \n",
      "3  2023-01-04   0.598658   0.508571   0.898554   0.637430   0.826457      32   \n",
      "4  2023-01-05   0.156019   0.907566   0.606429   0.726091   0.320050      83   \n",
      "..        ...        ...        ...        ...        ...        ...     ...   \n",
      "95 2023-04-06   0.493796   0.349210   0.522243   0.930757   0.353352      63   \n",
      "96 2023-04-07   0.522733   0.725956   0.769994   0.858413   0.583656      97   \n",
      "97 2023-04-08   0.427541   0.897110   0.215821   0.428994   0.077735      37   \n",
      "98 2023-04-09   0.025419   0.887086   0.622890   0.750871   0.974395      49   \n",
      "99 2023-04-10   0.107891   0.779876   0.085347   0.754543   0.986211      97   \n",
      "\n",
      "    feature_1_scaled  feature_1_lag3  feature_2_scaled  feature_2_lag3  \\\n",
      "0          -0.321493             NaN         -1.591213             NaN   \n",
      "1           1.615296             NaN          0.472785             NaN   \n",
      "2           0.880076             NaN         -0.625959             NaN   \n",
      "3           0.431873        0.374540          0.036638        0.031429   \n",
      "4          -1.056045        0.950714          1.397881        0.636410   \n",
      "..               ...             ...               ...             ...   \n",
      "95          0.079380        0.760785         -0.507050        0.900418   \n",
      "96          0.176652        0.561277          0.778285        0.633101   \n",
      "97         -0.143332        0.770967          1.362208        0.339030   \n",
      "98         -1.495050        0.493796          1.328010        0.349210   \n",
      "99         -1.217823        0.522733          0.962242        0.725956   \n",
      "\n",
      "    feature_3_scaled  feature_3_lag3  feature_4_scaled  feature_4_lag3  \\\n",
      "0           0.424060             NaN         -1.497577             NaN   \n",
      "1          -1.477241             NaN          0.137009             NaN   \n",
      "2          -1.213159             NaN          0.168635             NaN   \n",
      "3           1.298292        0.642032          0.498483        0.051682   \n",
      "4           0.302726        0.084140          0.800615        0.531355   \n",
      "..               ...             ...               ...             ...   \n",
      "95          0.015820        0.822601          1.498058        0.372018   \n",
      "96          0.860156        0.360191          1.251528        0.776413   \n",
      "97         -1.028471        0.127061         -0.211806        0.340804   \n",
      "98          0.358827        0.522243          0.885058        0.930757   \n",
      "99         -1.473126        0.769994          0.897570        0.858413   \n",
      "\n",
      "    feature_5_scaled  feature_5_lag3  \n",
      "0          -1.296048             NaN  \n",
      "1           1.213139             NaN  \n",
      "2          -0.033878             NaN  \n",
      "3           0.974296        0.103124  \n",
      "4          -0.615178        0.902553  \n",
      "..               ...             ...  \n",
      "95         -0.510650        0.277381  \n",
      "96          0.212210        0.188121  \n",
      "97         -1.375737        0.463698  \n",
      "98          1.438631        0.353352  \n",
      "99          1.475718        0.583656  \n",
      "\n",
      "[100 rows x 17 columns]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: <function DataFrame.shift> is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "Please refer to https://modin.readthedocs.io/en/stable/supported_apis/defaulting_to_pandas.html for explanation.\n",
      "UserWarning: <function DataFrame.shift> is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "UserWarning: <function DataFrame.shift> is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "UserWarning: <function DataFrame.shift> is not currently supported by PandasOnRay, defaulting to pandas implementation.\n",
      "UserWarning: <function DataFrame.shift> is not currently supported by PandasOnRay, defaulting to pandas implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modin -> Executed Successfully, Transformed Data:\n",
      "     datetime  feature_1  feature_2  feature_3  feature_4  feature_5  target  \\\n",
      "0  2023-01-01   0.374540   0.031429   0.642032   0.051682   0.103124      62   \n",
      "1  2023-01-02   0.950714   0.636410   0.084140   0.531355   0.902553      16   \n",
      "2  2023-01-03   0.731994   0.314356   0.161629   0.540635   0.505252      72   \n",
      "3  2023-01-04   0.598658   0.508571   0.898554   0.637430   0.826457      32   \n",
      "4  2023-01-05   0.156019   0.907566   0.606429   0.726091   0.320050      83   \n",
      "..        ...        ...        ...        ...        ...        ...     ...   \n",
      "95 2023-04-06   0.493796   0.349210   0.522243   0.930757   0.353352      63   \n",
      "96 2023-04-07   0.522733   0.725956   0.769994   0.858413   0.583656      97   \n",
      "97 2023-04-08   0.427541   0.897110   0.215821   0.428994   0.077735      37   \n",
      "98 2023-04-09   0.025419   0.887086   0.622890   0.750871   0.974395      49   \n",
      "99 2023-04-10   0.107891   0.779876   0.085347   0.754543   0.986211      97   \n",
      "\n",
      "    feature_1_scaled  feature_1_lag3  feature_2_scaled  feature_2_lag3  \\\n",
      "0          -0.321493             NaN         -1.591213             NaN   \n",
      "1           1.615296             NaN          0.472785             NaN   \n",
      "2           0.880076             NaN         -0.625959             NaN   \n",
      "3           0.431873        0.374540          0.036638        0.031429   \n",
      "4          -1.056045        0.950714          1.397881        0.636410   \n",
      "..               ...             ...               ...             ...   \n",
      "95          0.079380        0.760785         -0.507050        0.900418   \n",
      "96          0.176652        0.561277          0.778285        0.633101   \n",
      "97         -0.143332        0.770967          1.362208        0.339030   \n",
      "98         -1.495050        0.493796          1.328010        0.349210   \n",
      "99         -1.217823        0.522733          0.962242        0.725956   \n",
      "\n",
      "    feature_3_scaled  feature_3_lag3  feature_4_scaled  feature_4_lag3  \\\n",
      "0           0.424060             NaN         -1.497577             NaN   \n",
      "1          -1.477241             NaN          0.137009             NaN   \n",
      "2          -1.213159             NaN          0.168635             NaN   \n",
      "3           1.298292        0.642032          0.498483        0.051682   \n",
      "4           0.302726        0.084140          0.800615        0.531355   \n",
      "..               ...             ...               ...             ...   \n",
      "95          0.015820        0.822601          1.498058        0.372018   \n",
      "96          0.860156        0.360191          1.251528        0.776413   \n",
      "97         -1.028471        0.127061         -0.211806        0.340804   \n",
      "98          0.358827        0.522243          0.885058        0.930757   \n",
      "99         -1.473126        0.769994          0.897570        0.858413   \n",
      "\n",
      "    feature_5_scaled  feature_5_lag3  \n",
      "0          -1.296048             NaN  \n",
      "1           1.213139             NaN  \n",
      "2          -0.033878             NaN  \n",
      "3           0.974296        0.103124  \n",
      "4          -0.615178        0.902553  \n",
      "..               ...             ...  \n",
      "95         -0.510650        0.277381  \n",
      "96          0.212210        0.188121  \n",
      "97         -1.375737        0.463698  \n",
      "98          1.438631        0.353352  \n",
      "99          1.475718        0.583656  \n",
      "\n",
      "[100 rows x 17 columns]\n",
      "\n",
      "pyarrow -> Executed Successfully, Transformed Data:\n",
      "pyarrow.Table\n",
      "datetime: timestamp[ns]\n",
      "feature_1: double\n",
      "feature_2: double\n",
      "feature_3: double\n",
      "feature_4: double\n",
      "feature_5: double\n",
      "target: int64\n",
      "feature_1_scaled: double\n",
      "feature_1_lag3: double\n",
      "feature_2_scaled: double\n",
      "feature_2_lag3: double\n",
      "feature_3_scaled: double\n",
      "feature_3_lag3: double\n",
      "feature_4_scaled: double\n",
      "feature_4_lag3: double\n",
      "feature_5_scaled: double\n",
      "feature_5_lag3: double\n",
      "----\n",
      "datetime: [[2023-01-01 00:00:00.000000000,2023-01-02 00:00:00.000000000,2023-01-03 00:00:00.000000000,2023-01-04 00:00:00.000000000,2023-01-05 00:00:00.000000000,...,2023-04-06 00:00:00.000000000,2023-04-07 00:00:00.000000000,2023-04-08 00:00:00.000000000,2023-04-09 00:00:00.000000000,2023-04-10 00:00:00.000000000]]\n",
      "feature_1: [[0.3745401188473625,0.9507143064099162,0.7319939418114051,0.5986584841970366,0.15601864044243652,...,0.49379559636439074,0.5227328293819941,0.42754101835854963,0.02541912674409519,0.10789142699330445]]\n",
      "feature_2: [[0.03142918568673425,0.6364104112637804,0.3143559810763267,0.5085706911647028,0.907566473926093,...,0.3492095746126609,0.7259556788702394,0.8971102599525771,0.8870864242651173,0.7798755458576239]]\n",
      "feature_3: [[0.6420316461542878,0.08413996499504883,0.16162871409461377,0.8985541885270792,0.6064290596595899,...,0.5222432600548044,0.7699935530986108,0.21582102749684318,0.6228904758190003,0.085347464993768]]\n",
      "feature_4: [[0.0516817211686077,0.531354631568148,0.5406351216101065,0.6374299014982066,0.7260913337226615,...,0.9307573256035647,0.8584127518430118,0.42899402737501835,0.7508710677914974,0.7545428740846823]]\n",
      "feature_5: [[0.10312386883593261,0.9025529066795667,0.5052523724478571,0.8264574661077416,0.32004960103061175,...,0.3533522280260528,0.5836561118508721,0.07773463696498484,0.9743948076661665,0.9862107444796029]]\n",
      "target: [[62,16,72,32,83,...,63,97,37,49,97]]\n",
      "feature_1_scaled: [[-0.32149253381634435,1.6152963609416058,0.8800756892140731,0.4318733240969416,-1.056044656727614,...,0.07938048250384727,0.17665195485253263,-0.14333190843375598,-1.4950502443638614,-1.2178225609725948]]\n",
      "feature_1_lag3: [[null,null,null,0.3745401188473625,0.9507143064099162,...,0.7607850486168974,0.5612771975694962,0.770967179954561,0.49379559636439074,0.5227328293819941]]\n",
      "feature_2_scaled: [[-1.5912133480589814,0.47278528858807944,-0.62595939427216,0.036637856505107154,1.3978813411470248,...,-0.5070503000723712,0.7782845378939408,1.3622081491975693,1.328010090946824,0.9622415389463301]]\n",
      "...\n",
      "\n",
      "polars -> Executed Successfully, Transformed Data:\n",
      "shape: (100, 17)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ datetime  ┆ feature_1 ┆ feature_2 ┆ feature_3 ┆ … ┆ feature_4 ┆ feature_4 ┆ feature_5 ┆ feature_ │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ _scaled   ┆ _lag3     ┆ _scaled   ┆ 5_lag3   │\n",
      "│ datetime[ ┆ f64       ┆ f64       ┆ f64       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ ns]       ┆           ┆           ┆           ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 2023-01-0 ┆ 0.37454   ┆ 0.031429  ┆ 0.642032  ┆ … ┆ -1.497577 ┆ null      ┆ -1.296048 ┆ null     │\n",
      "│ 1         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-01-0 ┆ 0.950714  ┆ 0.63641   ┆ 0.08414   ┆ … ┆ 0.137009  ┆ null      ┆ 1.213139  ┆ null     │\n",
      "│ 2         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-01-0 ┆ 0.731994  ┆ 0.314356  ┆ 0.161629  ┆ … ┆ 0.168635  ┆ null      ┆ -0.033878 ┆ null     │\n",
      "│ 3         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-01-0 ┆ 0.598658  ┆ 0.508571  ┆ 0.898554  ┆ … ┆ 0.498483  ┆ 0.051682  ┆ 0.974296  ┆ 0.103124 │\n",
      "│ 4         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-01-0 ┆ 0.156019  ┆ 0.907566  ┆ 0.606429  ┆ … ┆ 0.800615  ┆ 0.531355  ┆ -0.615178 ┆ 0.902553 │\n",
      "│ 5         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
      "│ 2023-04-0 ┆ 0.493796  ┆ 0.34921   ┆ 0.522243  ┆ … ┆ 1.498058  ┆ 0.372018  ┆ -0.51065  ┆ 0.277381 │\n",
      "│ 6         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-04-0 ┆ 0.522733  ┆ 0.725956  ┆ 0.769994  ┆ … ┆ 1.251528  ┆ 0.776413  ┆ 0.21221   ┆ 0.188121 │\n",
      "│ 7         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-04-0 ┆ 0.427541  ┆ 0.89711   ┆ 0.215821  ┆ … ┆ -0.211806 ┆ 0.340804  ┆ -1.375737 ┆ 0.463698 │\n",
      "│ 8         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-04-0 ┆ 0.025419  ┆ 0.887086  ┆ 0.62289   ┆ … ┆ 0.885058  ┆ 0.930757  ┆ 1.438631  ┆ 0.353352 │\n",
      "│ 9         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-04-1 ┆ 0.107891  ┆ 0.779876  ┆ 0.085347  ┆ … ┆ 0.89757   ┆ 0.858413  ┆ 1.475718  ┆ 0.583656 │\n",
      "│ 0         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "\n",
      "dask -> Executed Successfully, Transformed Data:\n",
      "Dask DataFrame Structure:\n",
      "                     datetime feature_1 feature_2 feature_3 feature_4 feature_5 target feature_1_scaled feature_1_lag3 feature_2_scaled feature_2_lag3 feature_3_scaled feature_3_lag3 feature_4_scaled feature_4_lag3 feature_5_scaled feature_5_lag3\n",
      "npartitions=2                                                                                                                                                                                                                                         \n",
      "0              datetime64[ns]   float64   float64   float64   float64   float64  int64          float64        float64          float64        float64          float64        float64          float64        float64          float64        float64\n",
      "50                        ...       ...       ...       ...       ...       ...    ...              ...            ...              ...            ...              ...            ...              ...            ...              ...            ...\n",
      "99                        ...       ...       ...       ...       ...       ...    ...              ...            ...              ...            ...              ...            ...              ...            ...              ...            ...\n",
      "Dask Name: assign, 92 expressions\n",
      "Expr=Assign(frame=df)\n",
      "\n",
      "\n",
      "--- Summary of Backend Compatibility ---\n",
      "{'Backend': 'pandas', 'Transformed Data':      datetime  feature_1  feature_2  feature_3  feature_4  feature_5  target  \\\n",
      "0  2023-01-01   0.374540   0.031429   0.642032   0.051682   0.103124      62   \n",
      "1  2023-01-02   0.950714   0.636410   0.084140   0.531355   0.902553      16   \n",
      "2  2023-01-03   0.731994   0.314356   0.161629   0.540635   0.505252      72   \n",
      "3  2023-01-04   0.598658   0.508571   0.898554   0.637430   0.826457      32   \n",
      "4  2023-01-05   0.156019   0.907566   0.606429   0.726091   0.320050      83   \n",
      "..        ...        ...        ...        ...        ...        ...     ...   \n",
      "95 2023-04-06   0.493796   0.349210   0.522243   0.930757   0.353352      63   \n",
      "96 2023-04-07   0.522733   0.725956   0.769994   0.858413   0.583656      97   \n",
      "97 2023-04-08   0.427541   0.897110   0.215821   0.428994   0.077735      37   \n",
      "98 2023-04-09   0.025419   0.887086   0.622890   0.750871   0.974395      49   \n",
      "99 2023-04-10   0.107891   0.779876   0.085347   0.754543   0.986211      97   \n",
      "\n",
      "    feature_1_scaled  feature_1_lag3  feature_2_scaled  feature_2_lag3  \\\n",
      "0          -0.321493             NaN         -1.591213             NaN   \n",
      "1           1.615296             NaN          0.472785             NaN   \n",
      "2           0.880076             NaN         -0.625959             NaN   \n",
      "3           0.431873        0.374540          0.036638        0.031429   \n",
      "4          -1.056045        0.950714          1.397881        0.636410   \n",
      "..               ...             ...               ...             ...   \n",
      "95          0.079380        0.760785         -0.507050        0.900418   \n",
      "96          0.176652        0.561277          0.778285        0.633101   \n",
      "97         -0.143332        0.770967          1.362208        0.339030   \n",
      "98         -1.495050        0.493796          1.328010        0.349210   \n",
      "99         -1.217823        0.522733          0.962242        0.725956   \n",
      "\n",
      "    feature_3_scaled  feature_3_lag3  feature_4_scaled  feature_4_lag3  \\\n",
      "0           0.424060             NaN         -1.497577             NaN   \n",
      "1          -1.477241             NaN          0.137009             NaN   \n",
      "2          -1.213159             NaN          0.168635             NaN   \n",
      "3           1.298292        0.642032          0.498483        0.051682   \n",
      "4           0.302726        0.084140          0.800615        0.531355   \n",
      "..               ...             ...               ...             ...   \n",
      "95          0.015820        0.822601          1.498058        0.372018   \n",
      "96          0.860156        0.360191          1.251528        0.776413   \n",
      "97         -1.028471        0.127061         -0.211806        0.340804   \n",
      "98          0.358827        0.522243          0.885058        0.930757   \n",
      "99         -1.473126        0.769994          0.897570        0.858413   \n",
      "\n",
      "    feature_5_scaled  feature_5_lag3  \n",
      "0          -1.296048             NaN  \n",
      "1           1.213139             NaN  \n",
      "2          -0.033878             NaN  \n",
      "3           0.974296        0.103124  \n",
      "4          -0.615178        0.902553  \n",
      "..               ...             ...  \n",
      "95         -0.510650        0.277381  \n",
      "96          0.212210        0.188121  \n",
      "97         -1.375737        0.463698  \n",
      "98          1.438631        0.353352  \n",
      "99          1.475718        0.583656  \n",
      "\n",
      "[100 rows x 17 columns], 'Executed Successfully': True}\n",
      "{'Backend': 'modin', 'Transformed Data':      datetime  feature_1  feature_2  feature_3  feature_4  feature_5  target  \\\n",
      "0  2023-01-01   0.374540   0.031429   0.642032   0.051682   0.103124      62   \n",
      "1  2023-01-02   0.950714   0.636410   0.084140   0.531355   0.902553      16   \n",
      "2  2023-01-03   0.731994   0.314356   0.161629   0.540635   0.505252      72   \n",
      "3  2023-01-04   0.598658   0.508571   0.898554   0.637430   0.826457      32   \n",
      "4  2023-01-05   0.156019   0.907566   0.606429   0.726091   0.320050      83   \n",
      "..        ...        ...        ...        ...        ...        ...     ...   \n",
      "95 2023-04-06   0.493796   0.349210   0.522243   0.930757   0.353352      63   \n",
      "96 2023-04-07   0.522733   0.725956   0.769994   0.858413   0.583656      97   \n",
      "97 2023-04-08   0.427541   0.897110   0.215821   0.428994   0.077735      37   \n",
      "98 2023-04-09   0.025419   0.887086   0.622890   0.750871   0.974395      49   \n",
      "99 2023-04-10   0.107891   0.779876   0.085347   0.754543   0.986211      97   \n",
      "\n",
      "    feature_1_scaled  feature_1_lag3  feature_2_scaled  feature_2_lag3  \\\n",
      "0          -0.321493             NaN         -1.591213             NaN   \n",
      "1           1.615296             NaN          0.472785             NaN   \n",
      "2           0.880076             NaN         -0.625959             NaN   \n",
      "3           0.431873        0.374540          0.036638        0.031429   \n",
      "4          -1.056045        0.950714          1.397881        0.636410   \n",
      "..               ...             ...               ...             ...   \n",
      "95          0.079380        0.760785         -0.507050        0.900418   \n",
      "96          0.176652        0.561277          0.778285        0.633101   \n",
      "97         -0.143332        0.770967          1.362208        0.339030   \n",
      "98         -1.495050        0.493796          1.328010        0.349210   \n",
      "99         -1.217823        0.522733          0.962242        0.725956   \n",
      "\n",
      "    feature_3_scaled  feature_3_lag3  feature_4_scaled  feature_4_lag3  \\\n",
      "0           0.424060             NaN         -1.497577             NaN   \n",
      "1          -1.477241             NaN          0.137009             NaN   \n",
      "2          -1.213159             NaN          0.168635             NaN   \n",
      "3           1.298292        0.642032          0.498483        0.051682   \n",
      "4           0.302726        0.084140          0.800615        0.531355   \n",
      "..               ...             ...               ...             ...   \n",
      "95          0.015820        0.822601          1.498058        0.372018   \n",
      "96          0.860156        0.360191          1.251528        0.776413   \n",
      "97         -1.028471        0.127061         -0.211806        0.340804   \n",
      "98          0.358827        0.522243          0.885058        0.930757   \n",
      "99         -1.473126        0.769994          0.897570        0.858413   \n",
      "\n",
      "    feature_5_scaled  feature_5_lag3  \n",
      "0          -1.296048             NaN  \n",
      "1           1.213139             NaN  \n",
      "2          -0.033878             NaN  \n",
      "3           0.974296        0.103124  \n",
      "4          -0.615178        0.902553  \n",
      "..               ...             ...  \n",
      "95         -0.510650        0.277381  \n",
      "96          0.212210        0.188121  \n",
      "97         -1.375737        0.463698  \n",
      "98          1.438631        0.353352  \n",
      "99          1.475718        0.583656  \n",
      "\n",
      "[100 rows x 17 columns], 'Executed Successfully': True}\n",
      "{'Backend': 'pyarrow', 'Transformed Data': pyarrow.Table\n",
      "datetime: timestamp[ns]\n",
      "feature_1: double\n",
      "feature_2: double\n",
      "feature_3: double\n",
      "feature_4: double\n",
      "feature_5: double\n",
      "target: int64\n",
      "feature_1_scaled: double\n",
      "feature_1_lag3: double\n",
      "feature_2_scaled: double\n",
      "feature_2_lag3: double\n",
      "feature_3_scaled: double\n",
      "feature_3_lag3: double\n",
      "feature_4_scaled: double\n",
      "feature_4_lag3: double\n",
      "feature_5_scaled: double\n",
      "feature_5_lag3: double\n",
      "----\n",
      "datetime: [[2023-01-01 00:00:00.000000000,2023-01-02 00:00:00.000000000,2023-01-03 00:00:00.000000000,2023-01-04 00:00:00.000000000,2023-01-05 00:00:00.000000000,...,2023-04-06 00:00:00.000000000,2023-04-07 00:00:00.000000000,2023-04-08 00:00:00.000000000,2023-04-09 00:00:00.000000000,2023-04-10 00:00:00.000000000]]\n",
      "feature_1: [[0.3745401188473625,0.9507143064099162,0.7319939418114051,0.5986584841970366,0.15601864044243652,...,0.49379559636439074,0.5227328293819941,0.42754101835854963,0.02541912674409519,0.10789142699330445]]\n",
      "feature_2: [[0.03142918568673425,0.6364104112637804,0.3143559810763267,0.5085706911647028,0.907566473926093,...,0.3492095746126609,0.7259556788702394,0.8971102599525771,0.8870864242651173,0.7798755458576239]]\n",
      "feature_3: [[0.6420316461542878,0.08413996499504883,0.16162871409461377,0.8985541885270792,0.6064290596595899,...,0.5222432600548044,0.7699935530986108,0.21582102749684318,0.6228904758190003,0.085347464993768]]\n",
      "feature_4: [[0.0516817211686077,0.531354631568148,0.5406351216101065,0.6374299014982066,0.7260913337226615,...,0.9307573256035647,0.8584127518430118,0.42899402737501835,0.7508710677914974,0.7545428740846823]]\n",
      "feature_5: [[0.10312386883593261,0.9025529066795667,0.5052523724478571,0.8264574661077416,0.32004960103061175,...,0.3533522280260528,0.5836561118508721,0.07773463696498484,0.9743948076661665,0.9862107444796029]]\n",
      "target: [[62,16,72,32,83,...,63,97,37,49,97]]\n",
      "feature_1_scaled: [[-0.32149253381634435,1.6152963609416058,0.8800756892140731,0.4318733240969416,-1.056044656727614,...,0.07938048250384727,0.17665195485253263,-0.14333190843375598,-1.4950502443638614,-1.2178225609725948]]\n",
      "feature_1_lag3: [[null,null,null,0.3745401188473625,0.9507143064099162,...,0.7607850486168974,0.5612771975694962,0.770967179954561,0.49379559636439074,0.5227328293819941]]\n",
      "feature_2_scaled: [[-1.5912133480589814,0.47278528858807944,-0.62595939427216,0.036637856505107154,1.3978813411470248,...,-0.5070503000723712,0.7782845378939408,1.3622081491975693,1.328010090946824,0.9622415389463301]]\n",
      "..., 'Executed Successfully': True}\n",
      "{'Backend': 'polars', 'Transformed Data': shape: (100, 17)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ datetime  ┆ feature_1 ┆ feature_2 ┆ feature_3 ┆ … ┆ feature_4 ┆ feature_4 ┆ feature_5 ┆ feature_ │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ _scaled   ┆ _lag3     ┆ _scaled   ┆ 5_lag3   │\n",
      "│ datetime[ ┆ f64       ┆ f64       ┆ f64       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ ns]       ┆           ┆           ┆           ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 2023-01-0 ┆ 0.37454   ┆ 0.031429  ┆ 0.642032  ┆ … ┆ -1.497577 ┆ null      ┆ -1.296048 ┆ null     │\n",
      "│ 1         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-01-0 ┆ 0.950714  ┆ 0.63641   ┆ 0.08414   ┆ … ┆ 0.137009  ┆ null      ┆ 1.213139  ┆ null     │\n",
      "│ 2         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-01-0 ┆ 0.731994  ┆ 0.314356  ┆ 0.161629  ┆ … ┆ 0.168635  ┆ null      ┆ -0.033878 ┆ null     │\n",
      "│ 3         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-01-0 ┆ 0.598658  ┆ 0.508571  ┆ 0.898554  ┆ … ┆ 0.498483  ┆ 0.051682  ┆ 0.974296  ┆ 0.103124 │\n",
      "│ 4         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-01-0 ┆ 0.156019  ┆ 0.907566  ┆ 0.606429  ┆ … ┆ 0.800615  ┆ 0.531355  ┆ -0.615178 ┆ 0.902553 │\n",
      "│ 5         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
      "│ 2023-04-0 ┆ 0.493796  ┆ 0.34921   ┆ 0.522243  ┆ … ┆ 1.498058  ┆ 0.372018  ┆ -0.51065  ┆ 0.277381 │\n",
      "│ 6         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-04-0 ┆ 0.522733  ┆ 0.725956  ┆ 0.769994  ┆ … ┆ 1.251528  ┆ 0.776413  ┆ 0.21221   ┆ 0.188121 │\n",
      "│ 7         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-04-0 ┆ 0.427541  ┆ 0.89711   ┆ 0.215821  ┆ … ┆ -0.211806 ┆ 0.340804  ┆ -1.375737 ┆ 0.463698 │\n",
      "│ 8         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-04-0 ┆ 0.025419  ┆ 0.887086  ┆ 0.62289   ┆ … ┆ 0.885058  ┆ 0.930757  ┆ 1.438631  ┆ 0.353352 │\n",
      "│ 9         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 2023-04-1 ┆ 0.107891  ┆ 0.779876  ┆ 0.085347  ┆ … ┆ 0.89757   ┆ 0.858413  ┆ 1.475718  ┆ 0.583656 │\n",
      "│ 0         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘, 'Executed Successfully': True}\n",
      "{'Backend': 'dask', 'Transformed Data': Dask DataFrame Structure:\n",
      "                     datetime feature_1 feature_2 feature_3 feature_4 feature_5 target feature_1_scaled feature_1_lag3 feature_2_scaled feature_2_lag3 feature_3_scaled feature_3_lag3 feature_4_scaled feature_4_lag3 feature_5_scaled feature_5_lag3\n",
      "npartitions=2                                                                                                                                                                                                                                         \n",
      "0              datetime64[ns]   float64   float64   float64   float64   float64  int64          float64        float64          float64        float64          float64        float64          float64        float64          float64        float64\n",
      "50                        ...       ...       ...       ...       ...       ...    ...              ...            ...              ...            ...              ...            ...              ...            ...              ...            ...\n",
      "99                        ...       ...       ...       ...       ...       ...    ...              ...            ...              ...            ...              ...            ...              ...            ...              ...            ...\n",
      "Dask Name: assign, 92 expressions\n",
      "Expr=Assign(frame=df), 'Executed Successfully': True}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import modin.pandas as mpd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import dask.dataframe as dd\n",
    "import narwhals as nw\n",
    "from narwhals.typing import FrameT\n",
    "import numpy as np\n",
    "from temporalscope.core.core_utils import TEMPORALSCOPE_CORE_BACKEND_TYPES\n",
    "\n",
    "# Constants\n",
    "NUM_ROWS = 100\n",
    "SEED = 42\n",
    "\n",
    "# Data generator function\n",
    "def generate_data(num_rows: int = NUM_ROWS, backend: str = \"pandas\") -> pd.DataFrame:\n",
    "    \"\"\"Generates a time-series DataFrame with multiple features and a target column.\"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    data = pd.DataFrame({\n",
    "        \"datetime\": pd.date_range(start=\"2023-01-01\", periods=num_rows, freq=\"D\"),\n",
    "        **{f\"feature_{i}\": np.random.rand(num_rows) for i in range(1, 6)},  # Generating 5 random features\n",
    "        \"target\": np.random.randint(0, 100, num_rows)  # Random integer target\n",
    "    })\n",
    "    \n",
    "    # Convert to the specified backend\n",
    "    if backend == \"modin\":\n",
    "        return mpd.DataFrame(data)\n",
    "    elif backend == \"polars\":\n",
    "        return pl.DataFrame(data)\n",
    "    elif backend == \"pyarrow\":\n",
    "        return pa.Table.from_pandas(data)\n",
    "    elif backend == \"dask\":\n",
    "        return dd.from_pandas(data, npartitions=2)\n",
    "    return data  # Default to Pandas DataFrame\n",
    "\n",
    "# Narwhals-compatible function for scaling and lagging features\n",
    "@nw.narwhalify\n",
    "def scale_and_lag_features_nw(df: FrameT, lag_steps: int = 3) -> FrameT:\n",
    "    \"\"\"Applies scaling and lagging to each feature in a backend-agnostic way using Narwhals.\"\"\"\n",
    "    transformations = []\n",
    "    for col in df.columns:\n",
    "        if col.startswith(\"feature_\"):\n",
    "            transformations.extend([\n",
    "                ((nw.col(col) - nw.col(col).mean()) / nw.col(col).std()).alias(f\"{col}_scaled\"),\n",
    "                nw.col(col).shift(lag_steps).alias(f\"{col}_lag{lag_steps}\")\n",
    "            ])\n",
    "    return df.with_columns(*transformations)\n",
    "\n",
    "# Testing function across all backends\n",
    "def test_backends():\n",
    "    \"\"\"Tests `scale_and_lag_features_nw` on all supported TemporalScope backends.\"\"\"\n",
    "    results = []\n",
    "    for backend_name in TEMPORALSCOPE_CORE_BACKEND_TYPES.keys():\n",
    "        data_df = generate_data(backend=backend_name)\n",
    "        try:\n",
    "            transformed_df = scale_and_lag_features_nw(data_df)\n",
    "            results.append({\n",
    "                \"Backend\": backend_name,\n",
    "                \"Transformed Data\": transformed_df,\n",
    "                \"Executed Successfully\": True\n",
    "            })\n",
    "            print(f\"{backend_name} -> Executed Successfully, Transformed Data:\\n{transformed_df}\\n\")\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"Backend\": backend_name,\n",
    "                \"Executed Successfully\": False,\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "            print(f\"{backend_name} -> Failed with error: {e}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the backend compatibility tests\n",
    "    backend_results = test_backends()\n",
    "    print(\"\\n--- Summary of Backend Compatibility ---\")\n",
    "    for result in backend_results:\n",
    "        print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
