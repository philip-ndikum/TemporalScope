{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TemporalScope Tutorial: XAI Data Quality Validation\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This tutorial demonstrates how to validate time series data quality using research-backed thresholds. Data quality is critical for XAI (eXplainable AI) because poor quality data can lead to misleading explanations and unreliable models.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How to validate time series data using research-backed thresholds\n",
    "2. Why each validation check matters for XAI\n",
    "3. How to integrate validation into production pipelines\n",
    "\n",
    "## Research-Backed Validation\n",
    "\n",
    "Our validation thresholds come from key research papers:\n",
    "\n",
    "1. **Grinsztajn et al. (2022)**:\n",
    "   - Minimum 3,000 samples needed for reliable model training\n",
    "   - Feature-to-sample ratio should be < 1/10 to prevent overfitting\n",
    "   - Categorical features should have ≤ 20 unique values\n",
    "\n",
    "2. **Shwartz-Ziv et al. (2021)**:\n",
    "   - Maximum 50,000 samples for medium-sized datasets\n",
    "   - At least 4 features needed for meaningful complexity\n",
    "\n",
    "3. **Gorishniy et al. (2021)**:\n",
    "   - Keep features under 500 to avoid dimensionality issues\n",
    "   - Numerical features should have ≥ 10 unique values\n",
    "\n",
    "These thresholds help ensure your data is suitable for XAI analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import narwhals as nw\n",
    "\n",
    "from temporalscope.datasets.dataset_validator import DatasetValidator, ValidationResult\n",
    "\n",
    "# Create sample time series data\n",
    "data = pd.DataFrame({\n",
    "    'time': pd.date_range('2023-01-01', periods=5000),\n",
    "    'price': np.random.normal(100, 10, 5000),\n",
    "    'target': np.random.choice([0, 1], 5000)\n",
    "})\n",
    "\n",
    "# Initialize validator with research-backed thresholds\n",
    "validator = DatasetValidator(\n",
    "    time_col='time',\n",
    "    target_col='target',\n",
    "    min_samples=3000,\n",
    "    max_feature_ratio=0.1,\n",
    "    enable_warnings=True\n",
    ")\n",
    "\n",
    "# Run validation\n",
    "results = validator.fit_transform(data)\n",
    "\n",
    "print(\"Data Quality Validation Report:\")\n",
    "validator.print_report(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Validation Checks\n",
    "\n",
    "Each validation check has a specific purpose:\n",
    "\n",
    "1. **Sample Size** (min_samples=3000):\n",
    "   - WHY: Too few samples → unstable models\n",
    "   - WHY: Too many samples → computational issues\n",
    "\n",
    "2. **Feature Count**:\n",
    "   - WHY: Too few features → oversimplified model\n",
    "   - WHY: Too many features → curse of dimensionality\n",
    "\n",
    "3. **Feature Ratio** (max_feature_ratio=0.1):\n",
    "   - WHY: High ratio → risk of overfitting\n",
    "   - WHY: Based on statistical learning theory\n",
    "\n",
    "4. **Feature Variability**:\n",
    "   - WHY: Low variability → uninformative features\n",
    "   - WHY: Affects model's learning capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with quality issues\n",
    "problematic_data = pd.DataFrame({\n",
    "    'time': pd.date_range('2023-01-01', periods=1000),\n",
    "    'feature1': np.random.choice([1, 2], 1000),\n",
    "    'feature2': np.random.normal(0, 1, 1000),\n",
    "    'feature3': [None] * 100 + list(range(900)),\n",
    "    'target': np.random.choice([0, 1], 1000)\n",
    "})\n",
    "\n",
    "try:\n",
    "    # Initialize validator\n",
    "    validator = DatasetValidator(\n",
    "        time_col='time',\n",
    "        target_col='target',\n",
    "        min_samples=3000,\n",
    "        min_unique_values=10,\n",
    "        enable_warnings=True\n",
    "    )\n",
    "    \n",
    "    # Attempt validation\n",
    "    results = validator.fit_transform(problematic_data)\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(\"Validation Failed:\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\nWhy This Matters:\")\n",
    "    print(\"1. Too few samples (1000 < 3000) → unstable models\")\n",
    "    print(\"2. Missing values → unreliable predictions\")\n",
    "    print(\"3. Low feature variability → poor model learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Pipeline Integration\n",
    "\n",
    "The DatasetValidator uses Narwhals for backend-agnostic operations, making it suitable for production environments:\n",
    "\n",
    "1. **Production Environment**:\n",
    "   - Uses pandas + narwhals for efficient operations\n",
    "   - Lightweight deployment with minimal dependencies\n",
    "   - Consistent behavior in production\n",
    "\n",
    "2. **Test Environment**:\n",
    "   - Supports multiple backends via hatch\n",
    "   - Validates across different DataFrame implementations\n",
    "   - Ensures reliability across environments\n",
    "\n",
    "3. **Core Operations**:\n",
    "   - Uses @nw.narwhalify for backend conversions\n",
    "   - Pure Narwhals operations throughout\n",
    "   - Consistent behavior across supported types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nw.narwhalify\n",
    "def validate_production_data(df, time_col, target_col):\n",
    "    \"\"\"Production-ready validation function using Narwhals.\n",
    "    \n",
    "    Key Features:\n",
    "    1. Backend-agnostic operations\n",
    "    2. Proper error handling\n",
    "    3. Detailed logging\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame to validate\n",
    "        time_col: Name of time column\n",
    "        target_col: Name of target column\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (results, summary)\n",
    "    \"\"\"\n",
    "    validator = DatasetValidator(\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        min_samples=3000,\n",
    "        max_feature_ratio=0.1\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = validator.fit_transform(df)\n",
    "        summary = ValidationResult.get_validation_summary(results)\n",
    "        failed = ValidationResult.get_failed_checks(results)\n",
    "        \n",
    "        if failed:\n",
    "            for check_name, result in failed.items():\n",
    "                log_entry = result.to_log_entry()\n",
    "                print(f\"Failed Check: {check_name}\")\n",
    "                print(f\"Details: {log_entry}\")\n",
    "                \n",
    "                if result.severity == \"ERROR\":\n",
    "                    raise ValueError(f\"Critical validation failure: {check_name}\")\n",
    "        \n",
    "        return results, summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Validation failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test the production validation\n",
    "sample_data = pd.DataFrame({\n",
    "    'time': pd.date_range('2023-01-01', periods=5000),\n",
    "    'value': np.random.normal(0, 1, 5000),\n",
    "    'target': np.random.choice([0, 1], 5000)\n",
    "})\n",
    "\n",
    "results, summary = validate_production_data(sample_data, 'time', 'target')\n",
    "print(\"\\nValidation Summary:\")\n",
    "print(f\"Total Checks: {summary['total_checks']}\")\n",
    "print(f\"Failed Checks: {summary['failed_checks']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Pipeline Integration**:\n",
    "   ```python\n",
    "   # Airflow DAG Example\n",
    "   with DAG('data_validation_pipeline') as dag:\n",
    "       validate_task = PythonOperator(\n",
    "           task_id='validate_dataframe',\n",
    "           python_callable=validate_production_data,\n",
    "           op_kwargs={\n",
    "               'df': '{{ task_instance.xcom_pull(task_ids=\"load_data\") }}',\n",
    "               'time_col': 'timestamp',\n",
    "               'target_col': 'target'\n",
    "           }\n",
    "       )\n",
    "   ```\n",
    "\n",
    "2. **Monitoring Setup**:\n",
    "   - Track validation metrics over time\n",
    "   - Set up alerts for critical failures\n",
    "   - Monitor feature drift\n",
    "\n",
    "3. **Threshold Configuration**:\n",
    "   - Start with research-backed defaults\n",
    "   - Adjust based on domain requirements\n",
    "   - Document threshold decisions\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Define clear failure policies\n",
    "   - Set up fallback procedures\n",
    "   - Maintain audit trails\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "1. Grinsztajn et al. (2022) - Data quality thresholds\n",
    "2. Shwartz-Ziv et al. (2021) - Dataset size guidelines\n",
    "3. Gorishniy et al. (2021) - Feature complexity analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
