{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TemporalScope Tutorial: XAI Data Quality Validation\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This tutorial demonstrates how to validate time series data quality using research-backed thresholds. Data quality is critical for XAI (eXplainable AI) because poor quality data can lead to misleading explanations and unreliable models.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How to validate time series data using research-backed thresholds\n",
    "2. Why each validation check matters for XAI\n",
    "3. How to integrate validation into production pipelines\n",
    "\n",
    "## Research-Backed Validation\n",
    "\n",
    "Our validation thresholds come from key research papers:\n",
    "\n",
    "1. **Grinsztajn et al. (2022)**:\n",
    "   - Minimum 3,000 samples needed for reliable model training\n",
    "   - Feature-to-sample ratio should be < 1/10 to prevent overfitting\n",
    "   - Categorical features should have ≤ 20 unique values\n",
    "\n",
    "2. **Shwartz-Ziv et al. (2021)**:\n",
    "   - Maximum 50,000 samples for medium-sized datasets\n",
    "   - At least 4 features needed for meaningful complexity\n",
    "\n",
    "3. **Gorishniy et al. (2021)**:\n",
    "   - Keep features under 500 to avoid dimensionality issues\n",
    "   - Numerical features should have ≥ 10 unique values\n",
    "\n",
    "These thresholds help ensure your data is suitable for XAI analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd  # For DataFrame operations\n",
    "import numpy as np   # For numerical operations\n",
    "\n",
    "# Import TemporalScope components\n",
    "from temporalscope.datasets.dataset_validator import DatasetValidator, ValidationResult\n",
    "\n",
    "# Note: We'll import other backends (polars, modin) when needed\n",
    "# This keeps the initial setup simple and focused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Data Validation\n",
    "\n",
    "Let's start with a simple example to understand the core validation concepts.\n",
    "\n",
    "### What We're Doing\n",
    "1. Create a sample time series dataset\n",
    "2. Set up the validator with research-backed thresholds\n",
    "3. Run validation checks\n",
    "4. Understand the validation results\n",
    "\n",
    "### Why It Matters\n",
    "- Poor data quality leads to unreliable XAI results\n",
    "- Research-backed thresholds ensure meaningful analysis\n",
    "- Early validation prevents downstream issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a sample time series dataset\n",
    "# - Using 5000 samples (meets minimum requirement of 3000)\n",
    "# - One feature plus target (meets minimum feature requirement)\n",
    "# - Daily timestamps for time series structure\n",
    "data = pd.DataFrame({\n",
    "    'time': pd.date_range('2023-01-01', periods=5000),  # Daily timestamps\n",
    "    'price': np.random.normal(100, 10, 5000),           # Continuous feature\n",
    "    'target': np.random.choice([0, 1], 5000)            # Binary target\n",
    "})\n",
    "\n",
    "# Step 2: Initialize validator with research-backed thresholds\n",
    "validator = DatasetValidator(\n",
    "    time_col='time',      # Column containing timestamps\n",
    "    target_col='target',  # Column to predict\n",
    "    \n",
    "    # Research-backed thresholds from Grinsztajn et al. (2022)\n",
    "    min_samples=3000,         # Minimum samples for reliable training\n",
    "    max_feature_ratio=0.1,    # Prevent overfitting\n",
    "    \n",
    "    # Enable warnings to see detailed messages\n",
    "    enable_warnings=True\n",
    ")\n",
    "\n",
    "# Step 3: Run validation\n",
    "results = validator.fit_transform(data)\n",
    "\n",
    "# Step 4: Print detailed report\n",
    "print(\"Data Quality Validation Report:\")\n",
    "validator.print_report(results)\n",
    "\n",
    "# Understanding the results:\n",
    "# - ✓ means the check passed\n",
    "# - ✗ means the check failed\n",
    "# - Details show specific metrics and thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Validation Checks\n",
    "\n",
    "Each validation check has a specific purpose:\n",
    "\n",
    "1. **Sample Size** (min_samples=3000):\n",
    "   - WHY: Too few samples → unstable models\n",
    "   - WHY: Too many samples → computational issues\n",
    "   \n",
    "2. **Feature Count**:\n",
    "   - WHY: Too few features → oversimplified model\n",
    "   - WHY: Too many features → curse of dimensionality\n",
    "   \n",
    "3. **Feature Ratio** (max_feature_ratio=0.1):\n",
    "   - WHY: High ratio → risk of overfitting\n",
    "   - WHY: Based on statistical learning theory\n",
    "   \n",
    "4. **Feature Variability**:\n",
    "   - WHY: Low variability → uninformative features\n",
    "   - WHY: Affects model's learning capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Handling Data Quality Issues\n",
    "\n",
    "Now let's see what happens with problematic data and how to interpret the warnings.\n",
    "\n",
    "### Common Issues\n",
    "1. Too few samples\n",
    "2. Missing values\n",
    "3. Low feature variability\n",
    "4. Improper feature-to-sample ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with quality issues\n",
    "problematic_data = pd.DataFrame({\n",
    "    'time': pd.date_range('2023-01-01', periods=1000),     # Too few samples\n",
    "    'feature1': np.random.choice([1, 2], 1000),            # Low variability\n",
    "    'feature2': np.random.normal(0, 1, 1000),              # Good variability\n",
    "    'feature3': [None] * 100 + list(range(900)),           # Missing values\n",
    "    'target': np.random.choice([0, 1], 1000)               # Binary target\n",
    "})\n",
    "\n",
    "try:\n",
    "    # Initialize validator\n",
    "    validator = DatasetValidator(\n",
    "        time_col='time',\n",
    "        target_col='target',\n",
    "        min_samples=3000,           # Will fail (only 1000 samples)\n",
    "        min_unique_values=10,       # Will fail for feature1\n",
    "        enable_warnings=True\n",
    "    )\n",
    "    \n",
    "    # Attempt validation\n",
    "    results = validator.fit_transform(problematic_data)\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(\"Validation Failed:\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\nWhy This Matters:\")\n",
    "    print(\"1. Too few samples (1000 < 3000) → unstable models\")\n",
    "    print(\"2. Missing values → unreliable predictions\")\n",
    "    print(\"3. Low feature variability → poor model learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Production Pipeline Integration\n",
    "\n",
    "Learn how to integrate validation into production workflows.\n",
    "\n",
    "### Key Concepts\n",
    "1. Quality gates in pipelines\n",
    "2. Monitoring and alerting\n",
    "3. Error handling and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_production_data(df, time_col, target_col):\n",
    "    \"\"\"Production-ready validation function.\n",
    "    \n",
    "    Key Features:\n",
    "    1. Proper error handling\n",
    "    2. Detailed logging\n",
    "    3. Monitoring integration\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame to validate\n",
    "        time_col: Name of time column\n",
    "        target_col: Name of target column\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (results, summary)\n",
    "    \"\"\"\n",
    "    # Initialize validator with production thresholds\n",
    "    validator = DatasetValidator(\n",
    "        time_col=time_col,\n",
    "        target_col=target_col,\n",
    "        min_samples=3000,     # Research-backed minimum\n",
    "        max_feature_ratio=0.1  # Prevent overfitting\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Run validation\n",
    "        results = validator.fit_transform(df)\n",
    "        \n",
    "        # Get monitoring metrics\n",
    "        summary = ValidationResult.get_validation_summary(results)\n",
    "        failed = ValidationResult.get_failed_checks(results)\n",
    "        \n",
    "        # Handle failures\n",
    "        if failed:\n",
    "            for check_name, result in failed.items():\n",
    "                # Get structured log entry\n",
    "                log_entry = result.to_log_entry()\n",
    "                \n",
    "                # Log failure details\n",
    "                print(f\"Failed Check: {check_name}\")\n",
    "                print(f\"Details: {log_entry}\")\n",
    "                \n",
    "                # Critical failures stop the pipeline\n",
    "                if result.severity == \"ERROR\":\n",
    "                    raise ValueError(f\"Critical validation failure: {check_name}\")\n",
    "        \n",
    "        return results, summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Validation failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test the production validation\n",
    "sample_data = pd.DataFrame({\n",
    "    'time': pd.date_range('2023-01-01', periods=5000),\n",
    "    'value': np.random.normal(0, 1, 5000),\n",
    "    'target': np.random.choice([0, 1], 5000)\n",
    "})\n",
    "\n",
    "results, summary = validate_production_data(sample_data, 'time', 'target')\n",
    "print(\"\\nValidation Summary:\")\n",
    "print(f\"Total Checks: {summary['total_checks']}\")\n",
    "print(f\"Failed Checks: {summary['failed_checks']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Production\n",
    "\n",
    "### 1. Pipeline Integration\n",
    "```python\n",
    "# Airflow DAG Example\n",
    "with DAG('data_validation_pipeline') as dag:\n",
    "    validate_task = PythonOperator(\n",
    "        task_id='validate_dataframe',\n",
    "        python_callable=validate_production_data,\n",
    "        op_kwargs={\n",
    "            'df': '{{ task_instance.xcom_pull(task_ids=\"load_data\") }}',\n",
    "            'time_col': 'timestamp',\n",
    "            'target_col': 'target'\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "### 2. Monitoring Setup\n",
    "- Track validation metrics over time\n",
    "- Set up alerts for critical failures\n",
    "- Monitor feature drift\n",
    "\n",
    "### 3. Threshold Configuration\n",
    "- Start with research-backed defaults\n",
    "- Adjust based on domain requirements\n",
    "- Document threshold decisions\n",
    "\n",
    "### 4. Error Handling\n",
    "- Define clear failure policies\n",
    "- Set up fallback procedures\n",
    "- Maintain audit trails\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "1. Grinsztajn et al. (2022) - Data quality thresholds\n",
    "2. Shwartz-Ziv et al. (2021) - Dataset size guidelines\n",
    "3. Gorishniy et al. (2021) - Feature complexity analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
